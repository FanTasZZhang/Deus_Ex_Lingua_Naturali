{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/showjoo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data = pd.read_csv('./IMDB_Dataset.csv')\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Training and Testing data by dividing the dataset as 4:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000,), (30000,), (10000,), (10000,), (10000,), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews=imdb_data.review[:30000]\n",
    "train_sentiments=imdb_data.sentiment[:30000]\n",
    "\n",
    "valid_reviews=imdb_data.review[30000:40000]\n",
    "valid_sentiments=imdb_data.sentiment[30000:40000]\n",
    "\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]\n",
    "\n",
    "train_reviews.shape,train_sentiments.shape,valid_reviews.shape,valid_sentiments.shape, test_reviews.shape,test_sentiments.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure that the split is balanced in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    15015\n",
       "negative    14985\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentiments.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = ToktokTokenizer()\n",
    "# stopword_list=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# #Stemming the text, e.g. am, are, is -> be\n",
    "# def simple_stemmer(text):\n",
    "#     ps=nltk.porter.PorterStemmer()\n",
    "#     text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "#     return text\n",
    "\n",
    "# train_reviews = train_reviews.apply(simple_stemmer)\n",
    "# test_reviews = test_reviews.apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stopwords to english\n",
    "# stop = set(stopwords.words('english'))\n",
    "# print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #removing the stopwords\n",
    "# def remove_stopwords(text, is_lower_case=False):\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     tokens = [token.strip() for token in tokens]\n",
    "#     if is_lower_case:\n",
    "#         filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "#     else:\n",
    "#         filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "#     filtered_text = ' '.join(filtered_tokens)    \n",
    "#     return filtered_text\n",
    "\n",
    "# train_reviews = train_reviews.apply(remove_stopwords)\n",
    "# test_reviews = test_reviews.apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果在max_df中超过百分之50的一个词其中大部分都在positive，positive indicator？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Naive bayes model\n",
    " https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method that take in the training dataset, then return the positive and negative words-log probability dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method that take in the training dataset, then return the positive and negative words log probability.\n",
    "Input: train_reviews: reviews (sentences) for training\n",
    "       train_sentiments: sentiments (label) for training\n",
    "       tfidf: boolean variable indicating whether using bow or tfidf\n",
    "       alpha: laplance smoothing variable, default to be 1.0\n",
    "       ngram_range: the scale of ngram model will be used, default = (1,1) unigram\n",
    "return: negative_word_log_prob_dict: dictionary that contains the word:log probability pair for negative class\n",
    "        positive_word_log_prob_dict: dictionary that contains the word:log probability pair for positive class\n",
    "        mnb: the trained multinomial naive bayes model, later can be used for testing\n",
    "        transformed_test_reviews: transformed test reviews that later can be used for testing\n",
    "        vec: either the tfidfVectorize build from tfidf model or the CountVectorizer build from Bag of word model.\n",
    "'''\n",
    "\n",
    "def generate_log_prob(train_reviews, train_sentiments, tfidf=False, alpha=1.0, ngram_range = (1,1)):\n",
    "\n",
    "    if (tfidf):\n",
    "        #Tfidf vectorizer\n",
    "        vec=TfidfVectorizer(use_idf=tfidf, ngram_range=ngram_range)\n",
    "        #transformed train reviews\n",
    "        transformed_train_reviews=vec.fit_transform(train_reviews)\n",
    "        #transformed test reviews\n",
    "        transformed_test_reviews=vec.transform(test_reviews)\n",
    "    else:\n",
    "        vec=CountVectorizer(ngram_range=ngram_range)\n",
    "        transformed_train_reviews=vec.fit_transform(train_reviews)\n",
    "        transformed_test_reviews=vec.transform(test_reviews)\n",
    "\n",
    "    #training the model\n",
    "    mnb = MultinomialNB(alpha=alpha)\n",
    "\n",
    "    #fitting the naive bayes for bag of words\n",
    "    mnb = mnb.fit(transformed_train_reviews, train_sentiments)\n",
    "    negative_log_prob = mnb.feature_log_prob_[0]\n",
    "    positive_log_prob = mnb.feature_log_prob_[1]\n",
    "\n",
    "    # Generate two dict: word:log_prob\n",
    "    negative_word_log_prob_dict = {}\n",
    "    positive_word_log_prob_dict = {}\n",
    "    for word, index in vec.vocabulary_.items():\n",
    "        negative_word_log_prob_dict[word] = negative_log_prob[index]\n",
    "        positive_word_log_prob_dict[word] = positive_log_prob[index]\n",
    "    \n",
    "    return negative_word_log_prob_dict, positive_word_log_prob_dict, mnb, transformed_train_reviews, transformed_test_reviews, vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Method that given input word, manually change the weight of the word in naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This method will take in a word:scale dict, then take in the negative and positive word:log_probability dict, manually change the weight of the words in the model and the dict\n",
    "Input: negative_word_change_scale: this is the word-scale dictionary, which specify in the negative class, how much the weight of the word should be changed, For example\n",
    "                                   if the value is 0.5, we will say the probability of the word in negative class should multiply 0.5, transformed into addition for log prob\n",
    "       positive_word_change_scale: this is the word-scale dictionary, which specify in the positive class, how much the weight of the word should be changed, For example\n",
    "                                   if the value is 2, we will say the probability of the word in negative class should multiply 2, transformed into addition for log prob\n",
    "       model: the trained naive bayes model, which the feature_log_prob_ attribute will be manually changed based on previous two params\n",
    "       negative_word_log_prob_dict: dictionary that contains the word:log probability pair for negative class, which some values will be changed\n",
    "       positive_word_log_prob_dict: dictionary that contains the word:log probability pair for positive class, which some values will be changed\n",
    "       vec: either the tfidfVectorize build from tfidf model or the CountVectorizer build from Bag of word model.\n",
    "return: negative_word_change_scale: The modified negative dict\n",
    "        positive_word_change_scale: The modified positive dict\n",
    "        model: The modified naive bayes model\n",
    "'''\n",
    "\n",
    "def change_weight(negative_word_change_scale, positive_word_change_scale, model, negative_word_log_prob_dict, positive_word_log_prob_dict, vec):\n",
    "    for word, scale in negative_word_change_scale.items():\n",
    "        # change the weight of words in negative and positive word:log_prob dict\n",
    "        negative_word_log_prob_dict[word] *= scale\n",
    "\n",
    "        # change the weight of words in the model\n",
    "        index_in_model = vec.vocabulary_[word]\n",
    "        model.feature_log_prob_[0][index_in_model] *= scale\n",
    "\n",
    "    for word, scale in positive_word_change_scale.items():\n",
    "        # change the weight of words in negative and positive word:log_prob dict\n",
    "        positive_word_log_prob_dict[word] *= scale\n",
    "\n",
    "        # change the weight of words in the model\n",
    "        index_in_model = vec.vocabulary_[word]\n",
    "        model.feature_log_prob_[1][index_in_model] *= scale\n",
    "\n",
    "    return negative_word_log_prob_dict, positive_word_log_prob_dict, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.4849066497880004, -2.4849066497880004)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(12),np.log(1/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=30000, stop=40000, step=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sentiments.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'addtion'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vec = {k:v for v,k in vec.vocabulary_.items()}\n",
    "reverse_vec[1963]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Method that given input 2 dict, 寻找每个单词正反作用的count，向反作用的单词添加小weight，减弱其作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def classify(positive_dict,negative_dict,count_dict:defaultdict,transformed_valid_reviews,valid_sentiments,vec,wrong_label):\n",
    "    rows = np.nonzero(transformed_valid_reviews)[0]\n",
    "    columns = np.nonzero(transformed_valid_reviews)[1]\n",
    "    indices = zip(rows,columns)\n",
    "    start_index = valid_sentiments.index.start\n",
    "    # find word using index\n",
    "    reverse_vec = {k:v for v,k in vec.vocabulary_.items()}\n",
    "    #travase - only wrong case\n",
    "    if wrong_label:\n",
    "        indices = filter(lambda index:index[0] in wrong_label,indices)\n",
    "        for row,column in indices:\n",
    "            label = valid_sentiments[start_index+row]\n",
    "            word = reverse_vec[column]\n",
    "            count = transformed_valid_reviews[row,column]\n",
    "            #compare\n",
    "            if label == 'positive':\n",
    "                #起正作用情况\n",
    "                if positive_dict[word] > negative_dict[word]:\n",
    "                    count_dict[word]['good'] += count\n",
    "                #起负作用情况\n",
    "                elif positive_dict[word] < negative_dict[word]:\n",
    "                    count_dict[word]['bad'] += count\n",
    "            elif label == 'negative':\n",
    "                #起正作用情况\n",
    "                if positive_dict[word] < negative_dict[word]:\n",
    "                    count_dict[word]['good'] += count\n",
    "                #起负作用情况\n",
    "                elif positive_dict[word] > negative_dict[word]:\n",
    "                    count_dict[word]['bad'] += count\n",
    "    else: # all case\n",
    "        for row,column in indices:\n",
    "            label = valid_sentiments[start_index+row]\n",
    "            word = reverse_vec[column]\n",
    "            count = transformed_valid_reviews[row,column]\n",
    "            #compare\n",
    "            if label == 'positive':\n",
    "                #起正作用情况\n",
    "                if positive_dict[word] > negative_dict[word]:\n",
    "                    count_dict[word]['good'] += count\n",
    "                #起负作用情况\n",
    "                elif positive_dict[word] < negative_dict[word]:\n",
    "                    count_dict[word]['bad'] += count\n",
    "            elif label == 'negative':\n",
    "                #起正作用情况\n",
    "                if positive_dict[word] < negative_dict[word]:\n",
    "                    count_dict[word]['good'] += count\n",
    "                #起负作用情况\n",
    "                elif positive_dict[word] > negative_dict[word]:\n",
    "                    count_dict[word]['bad'] += count        \n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_count_wrong(positive_dict,negative_dict,transformed_valid_reviews,valid_sentiments,vec,wrong_label = []) -> dict:\n",
    "    count_dict = defaultdict(lambda: defaultdict(int))\n",
    "    count_dict = classify(positive_dict,negative_dict,count_dict,transformed_valid_reviews,valid_sentiments,vec,wrong_label)\n",
    "    #initialize reweight_dict\n",
    "    #reweight_dict = defaultdict(lambda: 1)\n",
    "    #filter bad > good, hyperparameter - set 怎么算要reweight\n",
    "    new_count_dict = {i:count_dict[i] for i in filter(lambda k:count_dict[k]['bad'] > count_dict[k]['good'],count_dict.keys())}\n",
    "    #return count_dict, hyperparameter - set reweight成多少\n",
    "    #transfer to bad+good/bad-good\n",
    "    transfer_count_dict = {k:(new_count_dict[k]['bad'] + new_count_dict[k]['good'])/(new_count_dict[k]['bad'] - new_count_dict[k]['good']) for k in new_count_dict.keys()}\n",
    "    #normalization\n",
    "    d_range = max(transfer_count_dict.values()) -min(transfer_count_dict.values())\n",
    "    d_min = min(transfer_count_dict.values())\n",
    "    test_reweight_dict = {k:(v-d_min)/d_range for k,v in transfer_count_dict.items()}\n",
    "    return test_reweight_dict,count_dict\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the result using bag of word methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method 1 using on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos, mnb, transformed_train_reviews, transformed_test_reviews, vec= generate_log_prob(train_reviews, train_sentiments, alpha = 0.05)\n",
    "transformed_valid_reviews=vec.transform(valid_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#产生调优测试案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reweight_dict,count_dict = reweight_count_wrong(pos,neg,transformed_valid_reviews,valid_sentiments,vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reweight_dict = reweight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_reweight_dict = {k:0.0001 for k,v in reweight_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#test_reweight_dict\\nnew_count_dict = {i:count_dict[i] for i in filter(lambda k:count_dict[k]['bad'] > count_dict[k]['good'],count_dict.keys())} \\n#transfer to bad+good/bad-good\\ntransfer_count_dict = {k:(new_count_dict[k]['bad'] + new_count_dict[k]['good'])/(new_count_dict[k]['bad'] - new_count_dict[k]['good']) for k in new_count_dict.keys()}\\n#normalization\\nd_range = max(transfer_count_dict.values()) -min(transfer_count_dict.values())\\nd_min = min(transfer_count_dict.values())\\ntest_reweight_dict = {k:(v-d_min)/d_range for k,v in transfer_count_dict.items()}\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#test_reweight_dict\n",
    "new_count_dict = {i:count_dict[i] for i in filter(lambda k:count_dict[k]['bad'] > count_dict[k]['good'],count_dict.keys())} \n",
    "#transfer to bad+good/bad-good\n",
    "transfer_count_dict = {k:(new_count_dict[k]['bad'] + new_count_dict[k]['good'])/(new_count_dict[k]['bad'] - new_count_dict[k]['good']) for k in new_count_dict.keys()}\n",
    "#normalization\n",
    "d_range = max(transfer_count_dict.values()) -min(transfer_count_dict.values())\n",
    "d_min = min(transfer_count_dict.values())\n",
    "test_reweight_dict = {k:(v-d_min)/d_range for k,v in transfer_count_dict.items()}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer_count_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_reweight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_reweight_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12469"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_reweight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'good': 22, 'bad': 34})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dict['silver']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block represent the length of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82453"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following two blocks shows the word:log_prob key value pair for each phrase in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following block use the trained model to do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_bow_predict = mnb.predict(transformed_test_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the corresponding test reviews log probability and original probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTICE: this probability was being normalized across all classes, so the log_probability was not the summation of log class probability and log probability of all phrases in the sentence. You can see that the summation of predict_proba() method is 1. The predict_log_proba() method calculate the log probability based on the normalized one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.00000000e+00, 8.17095253e-17]]),\n",
       " array([[  0.        , -37.04336109]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict_proba(transformed_test_reviews[0]), mnb.predict_log_proba(transformed_test_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This predict() method will return the predict value of given transformed review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['negative'], dtype='<U8'), array(['negative'], dtype='<U8'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict(transformed_test_reviews[0]), mnb.predict(transformed_test_reviews[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class_log_prior_ attribute of the MultinomialNB() model gives the prior class probability. In the order of the classes_ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['negative', 'positive'], dtype='<U8'),\n",
       " array([-0.69279724, -0.69349724]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.classes_, mnb.class_log_prior_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put the entire training set into the trained Naive Bayes model, and ge the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.843\n"
     ]
    }
   ],
   "source": [
    "mnb_bow_predict = mnb.predict(transformed_test_reviews)\n",
    "mnb_bow_score = accuracy_score(test_sentiments, mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.87879434,  -9.67537681, -14.97393224, ..., -14.97393224,\n",
       "        -14.97393224, -14.97393224],\n",
       "       [-11.04874497, -10.00137729, -18.03438678, ..., -18.03438678,\n",
       "        -18.03438678, -18.03438678]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.8431\n"
     ]
    }
   ],
   "source": [
    "#测valid\n",
    "mnb_bow_predict = mnb.predict(transformed_valid_reviews)\n",
    "mnb_bow_score = accuracy_score(valid_sentiments, mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.847\n"
     ]
    }
   ],
   "source": [
    "#change weight to test accuracy change\n",
    "new_neg, new_pos, new_mnb = change_weight(test_reweight_dict, test_reweight_dict, mnb, neg, pos, vec)\n",
    "mnb_bow_predict = new_mnb.predict(transformed_test_reviews)\n",
    "mnb_bow_score = accuracy_score(test_sentiments, mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.09831978,  -9.67537681, -14.97393224, ..., -14.97393224,\n",
       "        -14.97393224, -14.97393224],\n",
       "       [ -0.09985575, -10.00137729, -18.03438678, ..., -18.03438678,\n",
       "        -18.03438678, -18.03438678]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mnb.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.8808\n"
     ]
    }
   ],
   "source": [
    "#测valid\n",
    "mnb_bow_predict = mnb.predict(transformed_valid_reviews)\n",
    "mnb_bow_score = accuracy_score(valid_sentiments, mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.83      0.88      0.85      4993\n",
      "    Negative       0.87      0.82      0.84      5007\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for bag of words \n",
    "mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_bow_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_labeled_NB_BOW = \"\"\n",
    "\n",
    "for i in range(mnb_bow_predict.size):\n",
    "    start_index = test_sentiments.index.start\n",
    "    if(mnb_bow_predict[i] != test_sentiments[start_index + i]):\n",
    "        wrong_labeled_NB_BOW += str(start_index + i) + \" \" + str(imdb_data['sentiment'][start_index + i]) + \" | \" + str(imdb_data['review'][start_index + i]) +  \"\\n\"\n",
    "\n",
    "f = open(\"Naive Bayes - Bag of Words Wrongly labeled sentences.txt\", \"w\")\n",
    "f.write(wrong_labeled_NB_BOW)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method 2 try on only wrong labeled set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.8488\n"
     ]
    }
   ],
   "source": [
    "#another version - only wrong labeled\n",
    "neg, pos, mnb, transformed_train_reviews, transformed_test_reviews, vec= generate_log_prob(train_reviews, train_sentiments, alpha = 0.05)\n",
    "mnb_bow_predict = mnb.predict(transformed_test_reviews)\n",
    "mnb_bow_score = accuracy_score(test_sentiments, mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.918925\n"
     ]
    }
   ],
   "source": [
    "mnb_bow_train = mnb.predict(transformed_train_reviews)\n",
    "mnb_bow_score = accuracy_score(train_sentiments, mnb_bow_train)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_labeled_train = []\n",
    "for i in range(mnb_bow_train.size):\n",
    "    start_index = train_sentiments.index.start\n",
    "    if(mnb_bow_train[i] != train_sentiments[start_index + i]):\n",
    "        wrong_labeled_train.append(start_index + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3243"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reweight_dict_wrong,count_dict_wrong = reweight_count_wrong(pos,neg,transformed_train_reviews,train_sentiments,vec,wrong_labeled_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change weight to test accuracy change\n",
    "new_neg, new_pos, new_mnb = change_weight(reweight_dict_wrong, reweight_dict_wrong, mnb, neg, pos, vec)\n",
    "mnb_bow_predict = new_mnb.predict(transformed_test_reviews)\n",
    "mnb_bow_score = accuracy_score(test_sentiments, mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the model using TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos, mnb, transformed_train_reviews,transformed_test_reviews, vec = generate_log_prob(train_reviews, train_sentiments, tfidf = True, alpha = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_tfidf_score : 0.8623\n"
     ]
    }
   ],
   "source": [
    "mnb_tfidf_predict = mnb.predict(transformed_test_reviews)\n",
    "mnb_tfidf_score = accuracy_score(test_sentiments, mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweight_dict_idf,count_dict_idf = reweight_count_wrong(pos,neg,transformed_train_reviews,train_sentiments,vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reweight_dict_idf = {k:0.5 for k,v in reweight_dict_idf.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_tfidf_score : 0.8491\n"
     ]
    }
   ],
   "source": [
    "#change weight to test accuracy change\n",
    "new_neg, new_pos, new_mnb = change_weight(test_reweight_dict_idf, test_reweight_dict_idf, mnb, neg, pos, vec)\n",
    "mnb_tfidf_predict = new_mnb.predict(transformed_test_reviews)\n",
    "mnb_tfidf_score = accuracy_score(test_sentiments, mnb_bow_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.85      0.87      0.86      4993\n",
      "    Negative       0.87      0.85      0.86      5007\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for TF-IDF\n",
    "mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_labeled_NB_TFIDF = \"\"\n",
    "for i in range(mnb_tfidf_predict.size):\n",
    "    start_index = test_sentiments.index.start\n",
    "    if(mnb_tfidf_predict[i] != test_sentiments[start_index + i]):\n",
    "        wrong_labeled_NB_TFIDF += str(start_index + i) + \" \" + str(imdb_data['sentiment'][start_index + i]) + \" | \" + str(imdb_data['review'][start_index + i]) +  \"\\n\"\n",
    "\n",
    "f = open(\"Naive Bayes - TF-IDF Wrongly labeled sentences.txt\", \"w\")\n",
    "f.write(wrong_labeled_NB_TFIDF)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "test_reviews.index.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data so it can feed into fasttext\n",
    "def transform_instance(doc, label):\n",
    "    processed_text = []\n",
    "    #Prefix the index-ed label with __label__\n",
    "    for i in range(doc.index.start, doc.index.stop):\n",
    "        cur_row = \"__label__\" + label[i] + \" \" + doc[i]\n",
    "        processed_text.append(cur_row)\n",
    "    return processed_text\n",
    "\n",
    "training_text = transform_instance(train_reviews, train_sentiments)\n",
    "test_text = transform_instance(test_reviews, test_sentiments)\n",
    "\n",
    "# Put the training and test dataset into file, so that the fasttext model can read them.\n",
    "f = open(\"fasttext.train\", \"w\")\n",
    "output = \"\"\n",
    "for text in training_text:\n",
    "    output += text\n",
    "    output += \"\\n\"\n",
    "f.write(output)\n",
    "f.close()\n",
    "\n",
    "f = open(\"fasttext.test\", \"w\")\n",
    "output = \"\"\n",
    "for text in test_text:\n",
    "    output += text\n",
    "    output += \"\\n\"\n",
    "f.write(output)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 10000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_text), len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input = \"fasttext.train\", lr=0.1, epoch=25, wordNgrams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model_movie.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 0.8942, 0.8942)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"fasttext.test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the misclassified examples by fasttext algorithm\n",
    "wrong_labeled_fasttext = \"\"\n",
    "for i in range(test_reviews.size):\n",
    "    start_index = test_sentiments.index.start\n",
    "    if(model.predict(test_reviews[start_index + i])[0][0] != str('__label__' + test_sentiments[start_index + i])):\n",
    "        wrong_labeled_fasttext += str(start_index + i) + \" \" + str(imdb_data['sentiment'][start_index + i]) + \" | \" + str(imdb_data['review'][start_index + i]) +  \"\\n\"\n",
    "\n",
    "f = open(\"fastText Wrongly labeled sentences.txt\", \"w\")\n",
    "f.write(wrong_labeled_fasttext)\n",
    "f.close\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with pytorch\n",
    "https://www.kaggle.com/code/fantaszzhang/deep-learning-for-sentiment-analysis/edit"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60d1cc28a4b92593bd5dc5bc27ccc3190a35517554a34efe05fccaa5f4e07df9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
