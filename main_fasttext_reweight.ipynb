{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\16514\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\16514\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data = pd.read_csv('./data/IMDB Dataset.csv')\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Training and Testing data by dividing the dataset as 4:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews=imdb_data.review[:40000]\n",
    "train_sentiments=imdb_data.sentiment[:40000]\n",
    "\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(reviews):\n",
    "    container = []\n",
    "    for review in reviews:\n",
    "        review = review.replace(\"<br />\", \"\")\n",
    "        for ele in string.punctuation:\n",
    "                if ele in review:\n",
    "                        review = review.replace(ele, \"\")\n",
    "        container.append(review)\n",
    "    return container\n",
    "\n",
    "def preprocess_sentiment(sentiments):\n",
    "    container = []\n",
    "    for sentiment in sentiments:\n",
    "        container.append(sentiment)\n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews=preprocess(train_reviews)\n",
    "train_sentiments=preprocess_sentiment(train_sentiments)\n",
    "\n",
    "test_reviews=preprocess(test_reviews)\n",
    "test_sentiments=preprocess_sentiment(test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train.txt\", \"w\") as f:\n",
    "    for review in train_reviews:\n",
    "        f.write(review)\n",
    "        f.write(\"\\n\")\n",
    "with open(\"data/test.txt\", \"w\") as f:\n",
    "    for review in test_reviews:\n",
    "        f.write(review)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext manually modify the sentence embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = fasttext.train_unsupervised(\"data/train.txt\")\n",
    "# model.save_model(\"data/train.bin\")\n",
    "model = fasttext.load_model(\"data/train.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45409"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.get_word_vector(\"the\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = pickle.load(open(\"data/weight.pickle\", \"rb\"))\n",
    "\n",
    "def transform_sentence_embedding(data, word_dict):\n",
    "    representations = []\n",
    "    for sentence in data:\n",
    "        nltk_tokens = sentence.strip().split()\n",
    "        sum = 0\n",
    "        for token in nltk_tokens:\n",
    "            if token in word_dict:\n",
    "                sum += model.get_word_vector(token) * word_dict[token]\n",
    "            else:\n",
    "                sum += model.get_word_vector(token)\n",
    "        representations.append(sum / len(nltk_tokens))\n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label(data):\n",
    "    representations = []\n",
    "    for label in data:\n",
    "        if(label == 'negative'):\n",
    "            representations.append(0)\n",
    "        else:\n",
    "            representations.append(1)\n",
    "    return representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in word_dict.items():\n",
    "    if v < 0:\n",
    "        word_dict[k] = -1\n",
    "    if v > 0 and v < 1:\n",
    "        word_dict[k] = 0\n",
    "    if v > 1:\n",
    "        word_dict[k] = 2\n",
    "\n",
    "fasttext_train_reviews = torch.Tensor(np.array(transform_sentence_embedding(train_reviews, word_dict)))\n",
    "fasttext_test_reviews = torch.Tensor(np.array(transform_sentence_embedding(test_reviews, word_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train_sentiments = nn.functional.one_hot(torch.Tensor(np.array(transform_label(train_sentiments))).to(torch.int64)).to(torch.float).cuda()\n",
    "fasttext_test_sentiments = nn.functional.one_hot(torch.Tensor(np.array(transform_label(test_sentiments))).to(torch.int64)).to(torch.float).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train_reviews = fasttext_train_reviews.cuda()\n",
    "fasttext_test_reviews = fasttext_test_reviews.cuda()\n",
    "fasttext_train_sentiments = fasttext_train_sentiments.cuda()\n",
    "fasttext_test_sentiments = fasttext_test_sentiments.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([40000, 100]),\n",
       " torch.Size([40000, 2]),\n",
       " torch.Size([10000, 100]),\n",
       " torch.Size([10000, 2]))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_train_reviews.shape, fasttext_train_sentiments.shape, fasttext_test_reviews.shape, fasttext_test_sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "\n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "        # Non-linearity\n",
    "        self.sigmoid_2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function  # LINEAR\n",
    "        out = self.fc1(x)\n",
    "\n",
    "        # Non-linearity  # NON-LINEAR\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # Linear function (readout)  # LINEAR\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        out = self.sigmoid_2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100\n",
    "hidden_dim = 50\n",
    "output_dim = 2\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_epochs = 500\n",
    "batch_size = 1024\n",
    "\n",
    "net = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch \t Loss train \t Loss test\n",
      "1 / 500\t 0.6841\t\t 0.6843\n",
      "2 / 500\t 0.6614\t\t 0.6620\n",
      "3 / 500\t 0.6304\t\t 0.6315\n",
      "4 / 500\t 0.6015\t\t 0.6032\n",
      "5 / 500\t 0.5761\t\t 0.5783\n",
      "6 / 500\t 0.5546\t\t 0.5573\n",
      "7 / 500\t 0.5365\t\t 0.5396\n",
      "8 / 500\t 0.5220\t\t 0.5253\n",
      "9 / 500\t 0.5100\t\t 0.5135\n",
      "10 / 500\t 0.5005\t\t 0.5043\n",
      "11 / 500\t 0.4920\t\t 0.4958\n",
      "12 / 500\t 0.4856\t\t 0.4896\n",
      "13 / 500\t 0.4802\t\t 0.4841\n",
      "14 / 500\t 0.4751\t\t 0.4791\n",
      "15 / 500\t 0.4714\t\t 0.4754\n",
      "16 / 500\t 0.4678\t\t 0.4717\n",
      "17 / 500\t 0.4647\t\t 0.4688\n",
      "18 / 500\t 0.4623\t\t 0.4664\n",
      "19 / 500\t 0.4599\t\t 0.4639\n",
      "20 / 500\t 0.4581\t\t 0.4622\n",
      "21 / 500\t 0.4570\t\t 0.4610\n",
      "22 / 500\t 0.4545\t\t 0.4586\n",
      "23 / 500\t 0.4530\t\t 0.4570\n",
      "24 / 500\t 0.4518\t\t 0.4558\n",
      "25 / 500\t 0.4503\t\t 0.4543\n",
      "26 / 500\t 0.4489\t\t 0.4529\n",
      "27 / 500\t 0.4481\t\t 0.4520\n",
      "28 / 500\t 0.4477\t\t 0.4517\n",
      "29 / 500\t 0.4464\t\t 0.4504\n",
      "30 / 500\t 0.4452\t\t 0.4491\n",
      "31 / 500\t 0.4444\t\t 0.4483\n",
      "32 / 500\t 0.4443\t\t 0.4482\n",
      "33 / 500\t 0.4430\t\t 0.4469\n",
      "34 / 500\t 0.4425\t\t 0.4463\n",
      "35 / 500\t 0.4434\t\t 0.4474\n",
      "36 / 500\t 0.4412\t\t 0.4449\n",
      "37 / 500\t 0.4407\t\t 0.4445\n",
      "38 / 500\t 0.4403\t\t 0.4440\n",
      "39 / 500\t 0.4398\t\t 0.4434\n",
      "40 / 500\t 0.4399\t\t 0.4436\n",
      "41 / 500\t 0.4390\t\t 0.4426\n",
      "42 / 500\t 0.4386\t\t 0.4421\n",
      "43 / 500\t 0.4382\t\t 0.4417\n",
      "44 / 500\t 0.4379\t\t 0.4413\n",
      "45 / 500\t 0.4375\t\t 0.4409\n",
      "46 / 500\t 0.4376\t\t 0.4410\n",
      "47 / 500\t 0.4370\t\t 0.4405\n",
      "48 / 500\t 0.4366\t\t 0.4401\n",
      "49 / 500\t 0.4368\t\t 0.4402\n",
      "50 / 500\t 0.4361\t\t 0.4393\n",
      "51 / 500\t 0.4362\t\t 0.4395\n",
      "52 / 500\t 0.4361\t\t 0.4392\n",
      "53 / 500\t 0.4353\t\t 0.4385\n",
      "54 / 500\t 0.4352\t\t 0.4383\n",
      "55 / 500\t 0.4352\t\t 0.4385\n",
      "56 / 500\t 0.4352\t\t 0.4385\n",
      "57 / 500\t 0.4346\t\t 0.4375\n",
      "58 / 500\t 0.4343\t\t 0.4374\n",
      "59 / 500\t 0.4342\t\t 0.4371\n",
      "60 / 500\t 0.4355\t\t 0.4383\n",
      "61 / 500\t 0.4342\t\t 0.4374\n",
      "62 / 500\t 0.4337\t\t 0.4365\n",
      "63 / 500\t 0.4336\t\t 0.4366\n",
      "64 / 500\t 0.4334\t\t 0.4362\n",
      "65 / 500\t 0.4340\t\t 0.4367\n",
      "66 / 500\t 0.4334\t\t 0.4364\n",
      "67 / 500\t 0.4330\t\t 0.4360\n",
      "68 / 500\t 0.4334\t\t 0.4360\n",
      "69 / 500\t 0.4333\t\t 0.4359\n",
      "70 / 500\t 0.4328\t\t 0.4357\n",
      "71 / 500\t 0.4326\t\t 0.4354\n",
      "72 / 500\t 0.4326\t\t 0.4351\n",
      "73 / 500\t 0.4325\t\t 0.4354\n",
      "74 / 500\t 0.4323\t\t 0.4348\n",
      "75 / 500\t 0.4320\t\t 0.4346\n",
      "76 / 500\t 0.4319\t\t 0.4345\n",
      "77 / 500\t 0.4319\t\t 0.4347\n",
      "78 / 500\t 0.4317\t\t 0.4342\n",
      "79 / 500\t 0.4321\t\t 0.4350\n",
      "80 / 500\t 0.4315\t\t 0.4341\n",
      "81 / 500\t 0.4314\t\t 0.4339\n",
      "82 / 500\t 0.4316\t\t 0.4339\n",
      "83 / 500\t 0.4313\t\t 0.4339\n",
      "84 / 500\t 0.4317\t\t 0.4340\n",
      "85 / 500\t 0.4315\t\t 0.4338\n",
      "86 / 500\t 0.4318\t\t 0.4340\n",
      "87 / 500\t 0.4313\t\t 0.4336\n",
      "88 / 500\t 0.4310\t\t 0.4333\n",
      "89 / 500\t 0.4309\t\t 0.4335\n",
      "90 / 500\t 0.4307\t\t 0.4330\n",
      "91 / 500\t 0.4306\t\t 0.4331\n",
      "92 / 500\t 0.4308\t\t 0.4330\n",
      "93 / 500\t 0.4304\t\t 0.4328\n",
      "94 / 500\t 0.4305\t\t 0.4327\n",
      "95 / 500\t 0.4304\t\t 0.4330\n",
      "96 / 500\t 0.4304\t\t 0.4326\n",
      "97 / 500\t 0.4302\t\t 0.4327\n",
      "98 / 500\t 0.4301\t\t 0.4324\n",
      "99 / 500\t 0.4301\t\t 0.4325\n",
      "100 / 500\t 0.4305\t\t 0.4326\n",
      "101 / 500\t 0.4306\t\t 0.4326\n",
      "102 / 500\t 0.4301\t\t 0.4326\n",
      "103 / 500\t 0.4298\t\t 0.4321\n",
      "104 / 500\t 0.4298\t\t 0.4320\n",
      "105 / 500\t 0.4297\t\t 0.4319\n",
      "106 / 500\t 0.4297\t\t 0.4318\n",
      "107 / 500\t 0.4304\t\t 0.4331\n",
      "108 / 500\t 0.4296\t\t 0.4317\n",
      "109 / 500\t 0.4295\t\t 0.4318\n",
      "110 / 500\t 0.4299\t\t 0.4324\n",
      "111 / 500\t 0.4299\t\t 0.4318\n",
      "112 / 500\t 0.4293\t\t 0.4314\n",
      "113 / 500\t 0.4293\t\t 0.4315\n",
      "114 / 500\t 0.4292\t\t 0.4314\n",
      "115 / 500\t 0.4297\t\t 0.4315\n",
      "116 / 500\t 0.4292\t\t 0.4315\n",
      "117 / 500\t 0.4295\t\t 0.4314\n",
      "118 / 500\t 0.4290\t\t 0.4311\n",
      "119 / 500\t 0.4289\t\t 0.4310\n",
      "120 / 500\t 0.4289\t\t 0.4309\n",
      "121 / 500\t 0.4288\t\t 0.4309\n",
      "122 / 500\t 0.4289\t\t 0.4310\n",
      "123 / 500\t 0.4289\t\t 0.4312\n",
      "124 / 500\t 0.4294\t\t 0.4319\n",
      "125 / 500\t 0.4288\t\t 0.4306\n",
      "126 / 500\t 0.4286\t\t 0.4306\n",
      "127 / 500\t 0.4286\t\t 0.4306\n",
      "128 / 500\t 0.4293\t\t 0.4317\n",
      "129 / 500\t 0.4290\t\t 0.4307\n",
      "130 / 500\t 0.4285\t\t 0.4304\n",
      "131 / 500\t 0.4285\t\t 0.4303\n",
      "132 / 500\t 0.4288\t\t 0.4311\n",
      "133 / 500\t 0.4289\t\t 0.4313\n",
      "134 / 500\t 0.4303\t\t 0.4331\n",
      "135 / 500\t 0.4283\t\t 0.4302\n",
      "136 / 500\t 0.4284\t\t 0.4305\n",
      "137 / 500\t 0.4286\t\t 0.4309\n",
      "138 / 500\t 0.4291\t\t 0.4315\n",
      "139 / 500\t 0.4284\t\t 0.4306\n",
      "140 / 500\t 0.4284\t\t 0.4301\n",
      "141 / 500\t 0.4283\t\t 0.4300\n",
      "142 / 500\t 0.4281\t\t 0.4299\n",
      "143 / 500\t 0.4280\t\t 0.4299\n",
      "144 / 500\t 0.4280\t\t 0.4298\n",
      "145 / 500\t 0.4281\t\t 0.4298\n",
      "146 / 500\t 0.4279\t\t 0.4298\n",
      "147 / 500\t 0.4281\t\t 0.4302\n",
      "148 / 500\t 0.4278\t\t 0.4297\n",
      "149 / 500\t 0.4280\t\t 0.4301\n",
      "150 / 500\t 0.4278\t\t 0.4296\n",
      "151 / 500\t 0.4279\t\t 0.4296\n",
      "152 / 500\t 0.4278\t\t 0.4298\n",
      "153 / 500\t 0.4277\t\t 0.4296\n",
      "154 / 500\t 0.4277\t\t 0.4294\n",
      "155 / 500\t 0.4276\t\t 0.4295\n",
      "156 / 500\t 0.4277\t\t 0.4297\n",
      "157 / 500\t 0.4281\t\t 0.4303\n",
      "158 / 500\t 0.4281\t\t 0.4295\n",
      "159 / 500\t 0.4275\t\t 0.4293\n",
      "160 / 500\t 0.4276\t\t 0.4292\n",
      "161 / 500\t 0.4280\t\t 0.4295\n",
      "162 / 500\t 0.4277\t\t 0.4298\n",
      "163 / 500\t 0.4274\t\t 0.4292\n",
      "164 / 500\t 0.4275\t\t 0.4294\n",
      "165 / 500\t 0.4275\t\t 0.4290\n",
      "166 / 500\t 0.4274\t\t 0.4294\n",
      "167 / 500\t 0.4277\t\t 0.4292\n",
      "168 / 500\t 0.4276\t\t 0.4297\n",
      "169 / 500\t 0.4278\t\t 0.4301\n",
      "170 / 500\t 0.4272\t\t 0.4289\n",
      "171 / 500\t 0.4272\t\t 0.4288\n",
      "172 / 500\t 0.4272\t\t 0.4288\n",
      "173 / 500\t 0.4283\t\t 0.4306\n",
      "174 / 500\t 0.4271\t\t 0.4288\n",
      "175 / 500\t 0.4271\t\t 0.4288\n",
      "176 / 500\t 0.4271\t\t 0.4290\n",
      "177 / 500\t 0.4271\t\t 0.4287\n",
      "178 / 500\t 0.4270\t\t 0.4286\n",
      "179 / 500\t 0.4272\t\t 0.4287\n",
      "180 / 500\t 0.4271\t\t 0.4290\n",
      "181 / 500\t 0.4272\t\t 0.4287\n",
      "182 / 500\t 0.4270\t\t 0.4289\n",
      "183 / 500\t 0.4270\t\t 0.4285\n",
      "184 / 500\t 0.4278\t\t 0.4300\n",
      "185 / 500\t 0.4272\t\t 0.4292\n",
      "186 / 500\t 0.4269\t\t 0.4285\n",
      "187 / 500\t 0.4269\t\t 0.4285\n",
      "188 / 500\t 0.4272\t\t 0.4286\n",
      "189 / 500\t 0.4268\t\t 0.4285\n",
      "190 / 500\t 0.4268\t\t 0.4284\n",
      "191 / 500\t 0.4268\t\t 0.4286\n",
      "192 / 500\t 0.4270\t\t 0.4284\n",
      "193 / 500\t 0.4268\t\t 0.4283\n",
      "194 / 500\t 0.4268\t\t 0.4286\n",
      "195 / 500\t 0.4272\t\t 0.4293\n",
      "196 / 500\t 0.4268\t\t 0.4285\n",
      "197 / 500\t 0.4267\t\t 0.4284\n",
      "198 / 500\t 0.4267\t\t 0.4286\n",
      "199 / 500\t 0.4269\t\t 0.4283\n",
      "200 / 500\t 0.4266\t\t 0.4283\n",
      "201 / 500\t 0.4269\t\t 0.4289\n",
      "202 / 500\t 0.4272\t\t 0.4286\n",
      "203 / 500\t 0.4268\t\t 0.4283\n",
      "204 / 500\t 0.4266\t\t 0.4281\n",
      "205 / 500\t 0.4268\t\t 0.4288\n",
      "206 / 500\t 0.4265\t\t 0.4283\n",
      "207 / 500\t 0.4267\t\t 0.4287\n",
      "208 / 500\t 0.4268\t\t 0.4289\n",
      "209 / 500\t 0.4266\t\t 0.4281\n",
      "210 / 500\t 0.4265\t\t 0.4281\n",
      "211 / 500\t 0.4264\t\t 0.4282\n",
      "212 / 500\t 0.4266\t\t 0.4282\n",
      "213 / 500\t 0.4264\t\t 0.4283\n",
      "214 / 500\t 0.4264\t\t 0.4281\n",
      "215 / 500\t 0.4281\t\t 0.4293\n",
      "216 / 500\t 0.4264\t\t 0.4282\n",
      "217 / 500\t 0.4264\t\t 0.4280\n",
      "218 / 500\t 0.4266\t\t 0.4280\n",
      "219 / 500\t 0.4264\t\t 0.4279\n",
      "220 / 500\t 0.4266\t\t 0.4280\n",
      "221 / 500\t 0.4264\t\t 0.4282\n",
      "222 / 500\t 0.4263\t\t 0.4280\n",
      "223 / 500\t 0.4270\t\t 0.4292\n",
      "224 / 500\t 0.4263\t\t 0.4280\n",
      "225 / 500\t 0.4266\t\t 0.4280\n",
      "226 / 500\t 0.4264\t\t 0.4283\n",
      "227 / 500\t 0.4263\t\t 0.4278\n",
      "228 / 500\t 0.4274\t\t 0.4285\n",
      "229 / 500\t 0.4262\t\t 0.4279\n",
      "230 / 500\t 0.4263\t\t 0.4282\n",
      "231 / 500\t 0.4261\t\t 0.4278\n",
      "232 / 500\t 0.4261\t\t 0.4277\n",
      "233 / 500\t 0.4263\t\t 0.4282\n",
      "234 / 500\t 0.4261\t\t 0.4277\n",
      "235 / 500\t 0.4263\t\t 0.4283\n",
      "236 / 500\t 0.4261\t\t 0.4277\n",
      "237 / 500\t 0.4261\t\t 0.4279\n",
      "238 / 500\t 0.4263\t\t 0.4282\n",
      "239 / 500\t 0.4267\t\t 0.4280\n",
      "240 / 500\t 0.4270\t\t 0.4283\n",
      "241 / 500\t 0.4263\t\t 0.4277\n",
      "242 / 500\t 0.4267\t\t 0.4281\n",
      "243 / 500\t 0.4260\t\t 0.4277\n",
      "244 / 500\t 0.4264\t\t 0.4284\n",
      "245 / 500\t 0.4260\t\t 0.4277\n",
      "246 / 500\t 0.4264\t\t 0.4285\n",
      "247 / 500\t 0.4261\t\t 0.4276\n",
      "248 / 500\t 0.4262\t\t 0.4277\n",
      "249 / 500\t 0.4275\t\t 0.4287\n",
      "250 / 500\t 0.4259\t\t 0.4276\n",
      "251 / 500\t 0.4259\t\t 0.4277\n",
      "252 / 500\t 0.4260\t\t 0.4279\n",
      "253 / 500\t 0.4261\t\t 0.4281\n",
      "254 / 500\t 0.4259\t\t 0.4276\n",
      "255 / 500\t 0.4259\t\t 0.4278\n",
      "256 / 500\t 0.4260\t\t 0.4275\n",
      "257 / 500\t 0.4260\t\t 0.4275\n",
      "258 / 500\t 0.4258\t\t 0.4276\n",
      "259 / 500\t 0.4259\t\t 0.4276\n",
      "260 / 500\t 0.4258\t\t 0.4276\n",
      "261 / 500\t 0.4265\t\t 0.4287\n",
      "262 / 500\t 0.4260\t\t 0.4275\n",
      "263 / 500\t 0.4258\t\t 0.4275\n",
      "264 / 500\t 0.4258\t\t 0.4276\n",
      "265 / 500\t 0.4261\t\t 0.4281\n",
      "266 / 500\t 0.4258\t\t 0.4274\n",
      "267 / 500\t 0.4258\t\t 0.4276\n",
      "268 / 500\t 0.4257\t\t 0.4274\n",
      "269 / 500\t 0.4257\t\t 0.4274\n",
      "270 / 500\t 0.4257\t\t 0.4273\n",
      "271 / 500\t 0.4274\t\t 0.4299\n",
      "272 / 500\t 0.4259\t\t 0.4274\n",
      "273 / 500\t 0.4257\t\t 0.4273\n",
      "274 / 500\t 0.4280\t\t 0.4290\n",
      "275 / 500\t 0.4257\t\t 0.4273\n",
      "276 / 500\t 0.4257\t\t 0.4274\n",
      "277 / 500\t 0.4264\t\t 0.4277\n",
      "278 / 500\t 0.4257\t\t 0.4274\n",
      "279 / 500\t 0.4267\t\t 0.4280\n",
      "280 / 500\t 0.4257\t\t 0.4274\n",
      "281 / 500\t 0.4259\t\t 0.4279\n",
      "282 / 500\t 0.4257\t\t 0.4273\n",
      "283 / 500\t 0.4257\t\t 0.4272\n",
      "284 / 500\t 0.4258\t\t 0.4278\n",
      "285 / 500\t 0.4256\t\t 0.4272\n",
      "286 / 500\t 0.4256\t\t 0.4272\n",
      "287 / 500\t 0.4271\t\t 0.4283\n",
      "288 / 500\t 0.4261\t\t 0.4275\n",
      "289 / 500\t 0.4259\t\t 0.4280\n",
      "290 / 500\t 0.4258\t\t 0.4274\n",
      "291 / 500\t 0.4256\t\t 0.4272\n",
      "292 / 500\t 0.4255\t\t 0.4274\n",
      "293 / 500\t 0.4260\t\t 0.4282\n",
      "294 / 500\t 0.4256\t\t 0.4275\n",
      "295 / 500\t 0.4261\t\t 0.4282\n",
      "296 / 500\t 0.4256\t\t 0.4272\n",
      "297 / 500\t 0.4255\t\t 0.4272\n",
      "298 / 500\t 0.4255\t\t 0.4272\n",
      "299 / 500\t 0.4256\t\t 0.4271\n",
      "300 / 500\t 0.4258\t\t 0.4272\n",
      "301 / 500\t 0.4259\t\t 0.4274\n",
      "302 / 500\t 0.4255\t\t 0.4275\n",
      "303 / 500\t 0.4256\t\t 0.4276\n",
      "304 / 500\t 0.4256\t\t 0.4273\n",
      "305 / 500\t 0.4258\t\t 0.4273\n",
      "306 / 500\t 0.4265\t\t 0.4289\n",
      "307 / 500\t 0.4254\t\t 0.4271\n",
      "308 / 500\t 0.4268\t\t 0.4293\n",
      "309 / 500\t 0.4254\t\t 0.4273\n",
      "310 / 500\t 0.4254\t\t 0.4273\n",
      "311 / 500\t 0.4254\t\t 0.4274\n",
      "312 / 500\t 0.4254\t\t 0.4273\n",
      "313 / 500\t 0.4260\t\t 0.4274\n",
      "314 / 500\t 0.4259\t\t 0.4274\n",
      "315 / 500\t 0.4260\t\t 0.4282\n",
      "316 / 500\t 0.4254\t\t 0.4274\n",
      "317 / 500\t 0.4277\t\t 0.4305\n",
      "318 / 500\t 0.4258\t\t 0.4274\n",
      "319 / 500\t 0.4259\t\t 0.4273\n",
      "320 / 500\t 0.4263\t\t 0.4276\n",
      "321 / 500\t 0.4253\t\t 0.4271\n",
      "322 / 500\t 0.4253\t\t 0.4270\n",
      "323 / 500\t 0.4262\t\t 0.4285\n",
      "324 / 500\t 0.4254\t\t 0.4275\n",
      "325 / 500\t 0.4256\t\t 0.4278\n",
      "326 / 500\t 0.4253\t\t 0.4272\n",
      "327 / 500\t 0.4253\t\t 0.4271\n",
      "328 / 500\t 0.4254\t\t 0.4276\n",
      "329 / 500\t 0.4257\t\t 0.4280\n",
      "330 / 500\t 0.4253\t\t 0.4273\n",
      "331 / 500\t 0.4252\t\t 0.4271\n",
      "332 / 500\t 0.4256\t\t 0.4278\n",
      "333 / 500\t 0.4253\t\t 0.4270\n",
      "334 / 500\t 0.4252\t\t 0.4270\n",
      "335 / 500\t 0.4252\t\t 0.4270\n",
      "336 / 500\t 0.4258\t\t 0.4273\n",
      "337 / 500\t 0.4252\t\t 0.4272\n",
      "338 / 500\t 0.4252\t\t 0.4271\n",
      "339 / 500\t 0.4252\t\t 0.4270\n",
      "340 / 500\t 0.4252\t\t 0.4270\n",
      "341 / 500\t 0.4252\t\t 0.4272\n",
      "342 / 500\t 0.4261\t\t 0.4286\n",
      "343 / 500\t 0.4251\t\t 0.4271\n",
      "344 / 500\t 0.4253\t\t 0.4275\n",
      "345 / 500\t 0.4251\t\t 0.4271\n",
      "346 / 500\t 0.4255\t\t 0.4271\n",
      "347 / 500\t 0.4251\t\t 0.4272\n",
      "348 / 500\t 0.4255\t\t 0.4271\n",
      "349 / 500\t 0.4266\t\t 0.4279\n",
      "350 / 500\t 0.4254\t\t 0.4271\n",
      "351 / 500\t 0.4254\t\t 0.4271\n",
      "352 / 500\t 0.4253\t\t 0.4270\n",
      "353 / 500\t 0.4251\t\t 0.4272\n",
      "354 / 500\t 0.4252\t\t 0.4270\n",
      "355 / 500\t 0.4253\t\t 0.4270\n",
      "356 / 500\t 0.4251\t\t 0.4270\n",
      "357 / 500\t 0.4258\t\t 0.4272\n",
      "358 / 500\t 0.4253\t\t 0.4270\n",
      "359 / 500\t 0.4251\t\t 0.4269\n",
      "360 / 500\t 0.4255\t\t 0.4278\n",
      "361 / 500\t 0.4262\t\t 0.4275\n",
      "362 / 500\t 0.4256\t\t 0.4280\n",
      "363 / 500\t 0.4252\t\t 0.4269\n",
      "364 / 500\t 0.4263\t\t 0.4290\n",
      "365 / 500\t 0.4250\t\t 0.4271\n",
      "366 / 500\t 0.4260\t\t 0.4286\n",
      "367 / 500\t 0.4250\t\t 0.4272\n",
      "368 / 500\t 0.4249\t\t 0.4270\n",
      "369 / 500\t 0.4254\t\t 0.4271\n",
      "370 / 500\t 0.4249\t\t 0.4271\n",
      "371 / 500\t 0.4281\t\t 0.4312\n",
      "372 / 500\t 0.4250\t\t 0.4272\n",
      "373 / 500\t 0.4249\t\t 0.4269\n",
      "374 / 500\t 0.4252\t\t 0.4275\n",
      "375 / 500\t 0.4249\t\t 0.4269\n",
      "376 / 500\t 0.4249\t\t 0.4270\n",
      "377 / 500\t 0.4254\t\t 0.4270\n",
      "378 / 500\t 0.4249\t\t 0.4269\n",
      "379 / 500\t 0.4249\t\t 0.4270\n",
      "380 / 500\t 0.4254\t\t 0.4271\n",
      "381 / 500\t 0.4253\t\t 0.4270\n",
      "382 / 500\t 0.4291\t\t 0.4301\n",
      "383 / 500\t 0.4251\t\t 0.4269\n",
      "384 / 500\t 0.4252\t\t 0.4270\n",
      "385 / 500\t 0.4248\t\t 0.4270\n",
      "386 / 500\t 0.4253\t\t 0.4279\n",
      "387 / 500\t 0.4248\t\t 0.4269\n",
      "388 / 500\t 0.4248\t\t 0.4270\n",
      "389 / 500\t 0.4249\t\t 0.4273\n",
      "390 / 500\t 0.4256\t\t 0.4272\n",
      "391 / 500\t 0.4249\t\t 0.4273\n",
      "392 / 500\t 0.4254\t\t 0.4281\n",
      "393 / 500\t 0.4255\t\t 0.4272\n",
      "394 / 500\t 0.4248\t\t 0.4269\n",
      "395 / 500\t 0.4248\t\t 0.4272\n",
      "396 / 500\t 0.4248\t\t 0.4268\n",
      "397 / 500\t 0.4252\t\t 0.4278\n",
      "398 / 500\t 0.4247\t\t 0.4269\n",
      "399 / 500\t 0.4266\t\t 0.4280\n",
      "400 / 500\t 0.4264\t\t 0.4279\n",
      "401 / 500\t 0.4247\t\t 0.4270\n",
      "402 / 500\t 0.4261\t\t 0.4290\n",
      "403 / 500\t 0.4255\t\t 0.4271\n",
      "404 / 500\t 0.4248\t\t 0.4268\n",
      "405 / 500\t 0.4248\t\t 0.4271\n",
      "406 / 500\t 0.4249\t\t 0.4274\n",
      "407 / 500\t 0.4247\t\t 0.4269\n",
      "408 / 500\t 0.4247\t\t 0.4268\n",
      "409 / 500\t 0.4247\t\t 0.4270\n",
      "410 / 500\t 0.4251\t\t 0.4269\n",
      "411 / 500\t 0.4246\t\t 0.4269\n",
      "412 / 500\t 0.4249\t\t 0.4274\n",
      "413 / 500\t 0.4246\t\t 0.4270\n",
      "414 / 500\t 0.4246\t\t 0.4268\n",
      "415 / 500\t 0.4247\t\t 0.4272\n",
      "416 / 500\t 0.4254\t\t 0.4282\n",
      "417 / 500\t 0.4252\t\t 0.4279\n",
      "418 / 500\t 0.4247\t\t 0.4273\n",
      "419 / 500\t 0.4248\t\t 0.4274\n",
      "420 / 500\t 0.4256\t\t 0.4273\n",
      "421 / 500\t 0.4250\t\t 0.4278\n",
      "422 / 500\t 0.4259\t\t 0.4275\n",
      "423 / 500\t 0.4245\t\t 0.4270\n",
      "424 / 500\t 0.4254\t\t 0.4272\n",
      "425 / 500\t 0.4246\t\t 0.4267\n",
      "426 / 500\t 0.4250\t\t 0.4278\n",
      "427 / 500\t 0.4249\t\t 0.4277\n",
      "428 / 500\t 0.4246\t\t 0.4268\n",
      "429 / 500\t 0.4245\t\t 0.4268\n",
      "430 / 500\t 0.4247\t\t 0.4269\n",
      "431 / 500\t 0.4266\t\t 0.4281\n",
      "432 / 500\t 0.4245\t\t 0.4269\n",
      "433 / 500\t 0.4246\t\t 0.4271\n",
      "434 / 500\t 0.4245\t\t 0.4271\n",
      "435 / 500\t 0.4245\t\t 0.4268\n",
      "436 / 500\t 0.4247\t\t 0.4274\n",
      "437 / 500\t 0.4249\t\t 0.4269\n",
      "438 / 500\t 0.4250\t\t 0.4270\n",
      "439 / 500\t 0.4269\t\t 0.4303\n",
      "440 / 500\t 0.4246\t\t 0.4273\n",
      "441 / 500\t 0.4244\t\t 0.4268\n",
      "442 / 500\t 0.4246\t\t 0.4272\n",
      "443 / 500\t 0.4246\t\t 0.4273\n",
      "444 / 500\t 0.4250\t\t 0.4280\n",
      "445 / 500\t 0.4244\t\t 0.4267\n",
      "446 / 500\t 0.4249\t\t 0.4278\n",
      "447 / 500\t 0.4244\t\t 0.4268\n",
      "448 / 500\t 0.4244\t\t 0.4267\n",
      "449 / 500\t 0.4250\t\t 0.4281\n",
      "450 / 500\t 0.4255\t\t 0.4287\n",
      "451 / 500\t 0.4249\t\t 0.4269\n",
      "452 / 500\t 0.4245\t\t 0.4273\n",
      "453 / 500\t 0.4256\t\t 0.4288\n",
      "454 / 500\t 0.4246\t\t 0.4268\n",
      "455 / 500\t 0.4245\t\t 0.4267\n",
      "456 / 500\t 0.4243\t\t 0.4268\n",
      "457 / 500\t 0.4247\t\t 0.4268\n",
      "458 / 500\t 0.4243\t\t 0.4269\n",
      "459 / 500\t 0.4243\t\t 0.4269\n",
      "460 / 500\t 0.4246\t\t 0.4274\n",
      "461 / 500\t 0.4258\t\t 0.4275\n",
      "462 / 500\t 0.4257\t\t 0.4288\n",
      "463 / 500\t 0.4249\t\t 0.4269\n",
      "464 / 500\t 0.4243\t\t 0.4268\n",
      "465 / 500\t 0.4249\t\t 0.4269\n",
      "466 / 500\t 0.4247\t\t 0.4277\n",
      "467 / 500\t 0.4243\t\t 0.4270\n",
      "468 / 500\t 0.4255\t\t 0.4273\n",
      "469 / 500\t 0.4248\t\t 0.4268\n",
      "470 / 500\t 0.4242\t\t 0.4269\n",
      "471 / 500\t 0.4250\t\t 0.4281\n",
      "472 / 500\t 0.4242\t\t 0.4268\n",
      "473 / 500\t 0.4248\t\t 0.4269\n",
      "474 / 500\t 0.4252\t\t 0.4271\n",
      "475 / 500\t 0.4242\t\t 0.4267\n",
      "476 / 500\t 0.4242\t\t 0.4268\n",
      "477 / 500\t 0.4252\t\t 0.4284\n",
      "478 / 500\t 0.4247\t\t 0.4277\n",
      "479 / 500\t 0.4242\t\t 0.4270\n",
      "480 / 500\t 0.4241\t\t 0.4268\n",
      "481 / 500\t 0.4244\t\t 0.4266\n",
      "482 / 500\t 0.4243\t\t 0.4273\n",
      "483 / 500\t 0.4242\t\t 0.4270\n",
      "484 / 500\t 0.4246\t\t 0.4267\n",
      "485 / 500\t 0.4241\t\t 0.4268\n",
      "486 / 500\t 0.4241\t\t 0.4267\n",
      "487 / 500\t 0.4242\t\t 0.4265\n",
      "488 / 500\t 0.4245\t\t 0.4275\n",
      "489 / 500\t 0.4252\t\t 0.4284\n",
      "490 / 500\t 0.4252\t\t 0.4285\n",
      "491 / 500\t 0.4248\t\t 0.4280\n",
      "492 / 500\t 0.4261\t\t 0.4279\n",
      "493 / 500\t 0.4246\t\t 0.4268\n",
      "494 / 500\t 0.4240\t\t 0.4268\n",
      "495 / 500\t 0.4244\t\t 0.4276\n",
      "496 / 500\t 0.4240\t\t 0.4268\n",
      "497 / 500\t 0.4241\t\t 0.4266\n",
      "498 / 500\t 0.4245\t\t 0.4268\n",
      "499 / 500\t 0.4244\t\t 0.4267\n",
      "500 / 500\t 0.4243\t\t 0.4267\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "# Placeholder for loss\n",
    "track_loss = []\n",
    "\n",
    "print('Epoch', '\\t', 'Loss train', '\\t', 'Loss test')\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    # shuffle the permutation, so that the batches each time are different\n",
    "    shuffle_idx = np.random.permutation(len(fasttext_train_reviews))\n",
    "    #split the tensor into smaller batches\n",
    "    review_batches = torch.split(fasttext_train_reviews[shuffle_idx], batch_size)\n",
    "    sentiment_batches = torch.split(fasttext_train_sentiments[shuffle_idx], batch_size)\n",
    "    # Mini batches\n",
    "    for j in range(len(review_batches)):\n",
    "        review_batch = review_batches[j]\n",
    "\n",
    "        sentiment_batch = sentiment_batches[j]\n",
    "        \n",
    "        #where we train and build the network\n",
    "        output_train = net(review_batch)\n",
    "\n",
    "\n",
    "        loss = criterion(output_train, sentiment_batch)\n",
    "    \n",
    "        # compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Keep track of loss at each epoch\n",
    "    track_loss += [float(loss)]\n",
    "\n",
    "    loss_epoch = f'{i + 1} / {n_epochs}'\n",
    "    with torch.no_grad():\n",
    "        output_train = net(fasttext_train_reviews)\n",
    "        loss_train = criterion(output_train, fasttext_train_sentiments)\n",
    "        loss_epoch += f'\\t {loss_train:.4f}'\n",
    "\n",
    "        output_test = net(fasttext_test_reviews)\n",
    "        loss_test = criterion(output_test, fasttext_test_sentiments)\n",
    "        loss_epoch += f'\\t\\t {loss_test:.4f}'\n",
    "\n",
    "    print(loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_val = net(fasttext_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = []\n",
    "for (neg_prob, pos_prob) in predict_val:\n",
    "    if (neg_prob > pos_prob):\n",
    "        predict = \"negative\"\n",
    "    else:\n",
    "        predict = \"positive\"\n",
    "    predict_result.append(predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8817"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffnn_score = accuracy_score(test_sentiments, predict_result)\n",
    "ffnn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8812"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8812"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60d1cc28a4b92593bd5dc5bc27ccc3190a35517554a34efe05fccaa5f4e07df9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
