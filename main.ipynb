{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/williamchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/williamchen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = pd.read_csv('./IMDB_Dataset.csv')\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Training and Testing data by dividing the dataset as 4:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000,), (30000,), (10000,), (10000,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews=imdb_data.review[:30000]\n",
    "train_sentiments=imdb_data.sentiment[:30000]\n",
    "\n",
    "val_reviews = imdb_data.review[30000:40000]\n",
    "val_sentiments = imdb_data.sentiment[30000:40000]\n",
    "\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]\n",
    "\n",
    "train_reviews.shape,train_sentiments.shape, test_reviews.shape,test_sentiments.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure that the split is balanced in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiments.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Naive bayes model\n",
    " https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method that take in the training dataset, then return the positive and negative words-log probability dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method that take in the training dataset, then return the positive and negative words log probability.\n",
    "Input: train_reviews: reviews (sentences) for training\n",
    "       train_sentiments: sentiments (label) for training\n",
    "       tfidf: boolean variable indicating whether using bow or tfidf\n",
    "       alpha: laplance smoothing variable, default to be 1.0\n",
    "       ngram_range: the scale of ngram model will be used, default = (1,1) unigram\n",
    "return: negative_word_log_prob_dict: dictionary that contains the word:log probability pair for negative class\n",
    "        positive_word_log_prob_dict: dictionary that contains the word:log probability pair for positive class\n",
    "        mnb: the trained multinomial naive bayes model, later can be used for testing\n",
    "        transformed_test_reviews: transformed test reviews that later can be used for testing\n",
    "        vec: either the tfidfVectorize build from tfidf model or the CountVectorizer build from Bag of word model.\n",
    "'''\n",
    "\n",
    "def generate_log_prob(train_reviews, train_sentiments, tfidf=False, alpha=1.0, ngram_range = (1,1)):\n",
    "\n",
    "    if (tfidf):\n",
    "        #Tfidf vectorizer\n",
    "        vec=TfidfVectorizer(use_idf=tfidf, ngram_range=ngram_range)\n",
    "        #transformed train reviews\n",
    "        transformed_train_reviews=vec.fit_transform(train_reviews)\n",
    "        #transformed validation reviews\n",
    "        transformed_val_reviews=vec.transform(val_reviews)\n",
    "        #transformed test reviews\n",
    "        transformed_test_reviews=vec.transform(test_reviews)\n",
    "    else:\n",
    "        vec=CountVectorizer(ngram_range=(1,1))\n",
    "        transformed_train_reviews=vec.fit_transform(train_reviews)\n",
    "        transformed_val_reviews=vec.transform(val_reviews)\n",
    "        transformed_test_reviews=vec.transform(test_reviews)\n",
    "\n",
    "    #training the model\n",
    "    mnb = MultinomialNB(alpha=alpha)\n",
    "\n",
    "    #fitting the naive bayes for bag of words\n",
    "    mnb = mnb.fit(transformed_train_reviews, train_sentiments)\n",
    "    negative_log_prob = mnb.feature_log_prob_[0]\n",
    "    positive_log_prob = mnb.feature_log_prob_[1]\n",
    "\n",
    "    # Generate two dict: word:log_prob\n",
    "    negative_word_log_prob_dict = {}\n",
    "    positive_word_log_prob_dict = {}\n",
    "    for word, index in vec.vocabulary_.items():\n",
    "        negative_word_log_prob_dict[word] = negative_log_prob[index]\n",
    "        positive_word_log_prob_dict[word] = positive_log_prob[index]\n",
    "    \n",
    "    return negative_word_log_prob_dict, positive_word_log_prob_dict, mnb, transformed_val_reviews, transformed_test_reviews, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 3 count wrong by value\n",
    "\n",
    "def calculate_reweight_dict(positive_prob_dict, negative_prob_dict, dataset, threshold=0.1):\n",
    "    wrongly_classified_token_dict = {} # counts how many times a token has negative impact on wrongly classified sentence\n",
    "    token_dict = {} # counts how many times a token has appeared in total\n",
    "    reweight_dict = {}\n",
    "    for sentence in dataset:\n",
    "        real_label = sentence[6:14]\n",
    "        tokens = sentence[17:].split(\" \")\n",
    "        for token in tokens:\n",
    "            if token not in token_dict:\n",
    "                token_dict[token] = 1\n",
    "            else:\n",
    "                token_dict[token] += 1\n",
    "        if real_label == \"positive\": # marked as negative\n",
    "            for token in tokens:\n",
    "                if token not in positive_prob_dict: continue\n",
    "                elif positive_prob_dict[token] < negative_prob_dict[token]:\n",
    "                    if token not in wrongly_classified_token_dict:\n",
    "                        wrongly_classified_token_dict[token] = 1\n",
    "                    else:\n",
    "                        wrongly_classified_token_dict[token] += 1\n",
    "        elif real_label == \"negative\": # marked as positive\n",
    "            for token in tokens:\n",
    "                if token not in positive_prob_dict: continue\n",
    "                elif negative_prob_dict[token] < positive_prob_dict[token]:\n",
    "                    if token not in wrongly_classified_token_dict:\n",
    "                        wrongly_classified_token_dict[token] = 1\n",
    "                    else:\n",
    "                        wrongly_classified_token_dict[token] += 1\n",
    "    # print(wrongly_classified_token_dict)\n",
    "    for word, prob in wrongly_classified_token_dict.items():\n",
    "        # print(word, prob)\n",
    "        weight = 0\n",
    "        if positive_prob_dict[word] < negative_prob_dict[word]:\n",
    "            weight = positive_prob_dict[word] - negative_prob_dict[word]\n",
    "        elif negative_prob_dict[word] < positive_prob_dict[word]:\n",
    "            weight = negative_prob_dict[word] - positive_prob_dict[word]\n",
    "        multiplier = 1 - wrongly_classified_token_dict[word] / token_dict[word]\n",
    "        if multiplier == 0:\n",
    "            reweight_dict[word] = 0\n",
    "        else:\n",
    "            reweight_dict[word] = np.exp(weight * multiplier)\n",
    "    return reweight_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method that given input word, manually change the weight of the word in naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This method will take in a word:scale dict, then take in the negative and positive word:log_probability dict, manually change the weight of the words in the model and the dict\n",
    "Input: word_change_scale: this is the word-scale dictionary, how much the weight of the word should be changed, For example if the value is 0.5, we will say \n",
    "                          the probability of the word in negative class should multiply 0.5, in original probability, we take power to the scale\n",
    "       model: the trained naive bayes model, which the feature_log_prob_ attribute will be manually changed based on previous two params\n",
    "       negative_word_log_prob_dict: dictionary that contains the word:log probability pair for negative class, which some values will be changed\n",
    "       positive_word_log_prob_dict: dictionary that contains the word:log probability pair for positive class, which some values will be changed\n",
    "       vec: either the tfidfVectorize build from tfidf model or the CountVectorizer build from Bag of word model.\n",
    "return: negative_word_change_scale: The modified negative dict\n",
    "        positive_word_change_scale: The modified positive dict\n",
    "        model: The modified naive bayes model\n",
    "'''\n",
    "\n",
    "def change_weight(word_change_scale, model, negative_word_log_prob_dict, positive_word_log_prob_dict, vec):\n",
    "    for word, scale in word_change_scale.items():\n",
    "        # change the weight of words in negative and positive word:log_prob dict\n",
    "        negative_word_log_prob_dict[word] *= scale\n",
    "        positive_word_log_prob_dict[word] *= scale\n",
    "\n",
    "        # change the weight of words in the model\n",
    "        index_in_model = vec.vocabulary_[word]\n",
    "        model.feature_log_prob_[0][index_in_model] *= scale\n",
    "        model.feature_log_prob_[1][index_in_model] *= scale\n",
    "\n",
    "    return negative_word_log_prob_dict, positive_word_log_prob_dict, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the result using bag of word methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos, mnb, transformed_test_reviews, vec= generate_log_prob(train_reviews, train_sentiments, alpha = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put the entire training set into the trained Naive Bayes model, and ge the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.8492\n"
     ]
    }
   ],
   "source": [
    "mnb_bow_predict = mnb.predict(transformed_test_reviews)\n",
    "mnb_bow_score = accuracy_score(test_sentiments, mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "# mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
    "# print(mnb_bow_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_labeled_NB_BOW = []\n",
    "pattern = r'[^A-Za-z0-9]+'\n",
    "\n",
    "for i in range(mnb_bow_predict.size):\n",
    "    start_index = test_sentiments.index.start\n",
    "    if(mnb_bow_predict[i] != test_sentiments[start_index + i]):\n",
    "        sentiment = str(imdb_data['sentiment'][start_index + i])\n",
    "        review = str(imdb_data['review'][start_index + i])\n",
    "        review = re.sub(pattern, \" \", review.lower())\n",
    "        wrong_labeled_NB_BOW.append(sentiment, review)\n",
    "\n",
    "reweight_dict = calculate_reweight_dict(pos, neg, wrong_labeled_NB_BOW)\n",
    "neg, pos, mnb = change_weight(reweight_dict, mnb, neg, pos, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.9166\n"
     ]
    }
   ],
   "source": [
    "mnb_bow_predict = mnb.predict(transformed_test_reviews)\n",
    "mnb_bow_score = accuracy_score(test_sentiments, mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.89      0.95      0.92      4993\n",
      "    Negative       0.94      0.89      0.91      5007\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for bag of words \n",
    "mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_bow_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the model using TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos, mnb, transformed_test_reviews, vec = generate_log_prob(train_reviews, train_sentiments, tfidf = True, alpha = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_tfidf_score : 0.8624\n"
     ]
    }
   ],
   "source": [
    "mnb_tfidf_predict = mnb.predict(transformed_test_reviews)\n",
    "mnb_tfidf_score = accuracy_score(test_sentiments, mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.85      0.87      0.86      4993\n",
      "    Negative       0.87      0.85      0.86      5007\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for TF-IDF\n",
    "mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_labeled_NB_TFIDF = \"\"\n",
    "for i in range(mnb_tfidf_predict.size):\n",
    "    start_index = test_sentiments.index.start\n",
    "    if(mnb_tfidf_predict[i] != test_sentiments[start_index + i]):\n",
    "        wrong_labeled_NB_TFIDF += str(start_index + i) + \" \" + str(imdb_data['sentiment'][start_index + i]) + \" | \" + str(imdb_data['review'][start_index + i]) +  \"\\n\"\n",
    "\n",
    "f = open(\"Naive Bayes - TF-IDF Wrongly labeled sentences.txt\", \"w\")\n",
    "f.write(wrong_labeled_NB_TFIDF)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Naive Bayes - Bag of Words Wrongly labeled sentences.txt\")\n",
    "dataset = []\n",
    "for line in f:\n",
    "    pattern = r'[^A-Za-z0-9]+'\n",
    "    line = re.sub(pattern, \" \", line.lower())\n",
    "    dataset.append(line)\n",
    "reweight_dict = calculate_reweight_dict(pos, neg, dataset)\n",
    "neg, pos, mnb = change_weight(reweight_dict, mnb, neg, pos, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_tfidf_score : 0.8928\n"
     ]
    }
   ],
   "source": [
    "mnb_tfidf_predict = mnb.predict(transformed_test_reviews)\n",
    "mnb_tfidf_score = accuracy_score(test_sentiments, mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.88      0.91      0.89      4993\n",
      "    Negative       0.91      0.88      0.89      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for TF-IDF\n",
    "mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "test_reviews.index.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #preprocess the data so it can feed into fasttext\n",
    "# def transform_instance(doc, label):\n",
    "#     processed_text = []\n",
    "#     #Prefix the index-ed label with __label__\n",
    "#     for i in range(doc.index.start, doc.index.stop):\n",
    "#         cur_row = \"__label__\" + label[i] + \" \" + doc[i]\n",
    "#         processed_text.append(cur_row)\n",
    "#     return processed_text\n",
    "\n",
    "# training_text = transform_instance(train_reviews, train_sentiments)\n",
    "# test_text = transform_instance(test_reviews, test_sentiments)\n",
    "\n",
    "# # Put the training and test dataset into file, so that the fasttext model can read them.\n",
    "# f = open(\"fasttext.train\", \"w\")\n",
    "# output = \"\"\n",
    "# for text in training_text:\n",
    "#     output += text\n",
    "#     output += \"\\n\"\n",
    "# f.write(output)\n",
    "# f.close()\n",
    "\n",
    "# f = open(\"fasttext.test\", \"w\")\n",
    "# output = \"\"\n",
    "# for text in test_text:\n",
    "#     output += text\n",
    "#     output += \"\\n\"\n",
    "# f.write(output)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 9M words\n",
      "Number of words:  190288\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 5014258 lr:  0.000000 avg.loss:  0.107517 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input = \"fasttext.train\", lr=0.1, epoch=45, wordNgrams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model_movie.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 0.8905, 0.8905)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"fasttext.test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the misclassified examples by fasttext algorithm\n",
    "wrong_labeled_fasttext = \"\"\n",
    "for i in range(test_reviews.size):\n",
    "    start_index = test_sentiments.index.start\n",
    "    if(model.predict(test_reviews[start_index + i])[0][0] != str('__label__' + test_sentiments[start_index + i])):\n",
    "        wrong_labeled_fasttext += str(start_index + i) + \" \" + str(imdb_data['sentiment'][start_index + i]) + \" | \" + str(imdb_data['review'][start_index + i]) +  \"\\n\"\n",
    "\n",
    "f = open(\"fastText Wrongly labeled sentences.txt\", \"w\")\n",
    "f.write(wrong_labeled_fasttext)\n",
    "f.close\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext manually modify the sentence embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/williamchen/Documents/GitHub/BaselineModel/main.ipynb Cell 61'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/williamchen/Documents/GitHub/BaselineModel/main.ipynb#ch0000052?line=0'>1</a>\u001b[0m train_reviews\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/williamchen/Documents/GitHub/BaselineModel/main.ipynb#ch0000052?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/williamchen/Documents/GitHub/BaselineModel/main.ipynb#ch0000052?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "train_reviews\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def output_train_reviews_to_doc(train_reviews):\n",
    "#     processed_doc = \"\"\n",
    "#     for review in train_reviews:\n",
    "#         processed_doc += review + \"\\n\"\n",
    "#     return processed_doc\n",
    "\n",
    "# f = open(\"wordRepresentation.train\", \"w\")\n",
    "# f.write(output_train_reviews_to_doc(train_reviews))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised(\"wordRepresentation.train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_word_vector(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each train and test review, use all the embeddings of words, calculate an average to represent the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence_embedding(data, word_list):\n",
    "    representations = []\n",
    "    for sentence in data:\n",
    "        nltk_tokens = nltk.word_tokenize(sentence)\n",
    "        sum = 0\n",
    "        count = 0\n",
    "        for token in nltk_tokens:\n",
    "            if token not in word_list:\n",
    "                sum += model.get_word_vector(token)\n",
    "                count += 1\n",
    "        representations.append(sum / count)\n",
    "    return representations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label(data):\n",
    "    representations = []\n",
    "    for label in data:\n",
    "        if(label == 'negative'):\n",
    "            representations.append(0)\n",
    "        else:\n",
    "            representations.append(1)\n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "fasttext_train_reviews = torch.Tensor(np.array(transform_sentence_embedding(train_reviews, word_list)))\n",
    "fasttext_test_reviews = torch.Tensor(np.array(transform_sentence_embedding(test_reviews, word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train_sentiments = nn.functional.one_hot(torch.Tensor(np.array(transform_label(train_sentiments))).to(torch.int64)).to(torch.float)\n",
    "fasttext_test_sentiments = nn.functional.one_hot(torch.Tensor(np.array(transform_label(test_sentiments))).to(torch.int64)).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train_reviews.shape, fasttext_train_sentiments.shape, fasttext_test_reviews.shape, fasttext_test_sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "\n",
    "        # Non-linearity\n",
    "        self.sigmoid_1 = nn.Sigmoid()\n",
    "\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "        # Non-linearity\n",
    "        self.sigmoid_2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function  # LINEAR\n",
    "        out = self.fc1(x)\n",
    "\n",
    "        # Non-linearity  # NON-LINEAR\n",
    "        out = self.sigmoid_1(out)\n",
    "\n",
    "        # Linear function (readout)  # LINEAR\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        out = self.sigmoid_2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100\n",
    "hidden_dim = 100\n",
    "output_dim = 2\n",
    "\n",
    "learning_rate = 0.1\n",
    "n_epochs = 40\n",
    "batch_size = 32\n",
    "\n",
    "net = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "# Placeholder for loss\n",
    "track_loss = []\n",
    "\n",
    "print('Epoch', '\\t', 'Loss train', '\\t', 'Loss test')\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    # shuffle the permutation, so that the batches each time are different\n",
    "    shuffle_idx = np.random.permutation(len(fasttext_train_reviews))\n",
    "    #split the tensor into smaller batches\n",
    "    review_batches = torch.split(fasttext_train_reviews[shuffle_idx], batch_size)\n",
    "    sentiment_batches = torch.split(fasttext_train_sentiments[shuffle_idx], batch_size)\n",
    "    # Mini batches\n",
    "    for j in range(len(review_batches)):\n",
    "        review_batch = review_batches[j]\n",
    "\n",
    "        sentiment_batch = sentiment_batches[j]\n",
    "        \n",
    "        #where we train and build the network\n",
    "        output_train = net(review_batch)\n",
    "\n",
    "\n",
    "        loss = criterion(output_train, sentiment_batch)\n",
    "    \n",
    "        # compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Keep track of loss at each epoch\n",
    "    track_loss += [float(loss)]\n",
    "\n",
    "    loss_epoch = f'{i + 1} / {n_epochs}'\n",
    "    with torch.no_grad():\n",
    "        output_train = net(fasttext_train_reviews)\n",
    "        loss_train = criterion(output_train, fasttext_train_sentiments)\n",
    "        loss_epoch += f'\\t {loss_train:.4f}'\n",
    "\n",
    "        output_test = net(fasttext_test_reviews)\n",
    "        loss_test = criterion(output_test, fasttext_test_sentiments)\n",
    "        loss_epoch += f'\\t\\t {loss_test:.4f}'\n",
    "\n",
    "    print(loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_val = net(fasttext_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = []\n",
    "for (neg_prob, pos_prob) in predict_val:\n",
    "    if (neg_prob > pos_prob):\n",
    "        predict = \"negative\"\n",
    "    else:\n",
    "        predict = \"positive\"\n",
    "    predict_result.append(predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn_score = accuracy_score(test_sentiments, predict_result)\n",
    "ffnn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60d1cc28a4b92593bd5dc5bc27ccc3190a35517554a34efe05fccaa5f4e07df9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
