{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = pd.read_csv('./IMDB_Dataset.csv')\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Training and Testing data by dividing the dataset as 4:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews=imdb_data.review[:40000]\n",
    "train_sentiments=imdb_data.sentiment[:40000]\n",
    "\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]\n",
    "\n",
    "train_reviews.shape,train_sentiments.shape, test_reviews.shape,test_sentiments.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure that the split is balanced in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiments.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = ToktokTokenizer()\n",
    "# stopword_list=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# #Stemming the text, e.g. am, are, is -> be\n",
    "# def simple_stemmer(text):\n",
    "#     ps=nltk.porter.PorterStemmer()\n",
    "#     text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "#     return text\n",
    "\n",
    "# train_reviews = train_reviews.apply(simple_stemmer)\n",
    "# test_reviews = test_reviews.apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stopwords to english\n",
    "# stop = set(stopwords.words('english'))\n",
    "# print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #removing the stopwords\n",
    "# def remove_stopwords(text, is_lower_case=False):\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     tokens = [token.strip() for token in tokens]\n",
    "#     if is_lower_case:\n",
    "#         filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "#     else:\n",
    "#         filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "#     filtered_text = ' '.join(filtered_tokens)    \n",
    "#     return filtered_text\n",
    "\n",
    "# train_reviews = train_reviews.apply(remove_stopwords)\n",
    "# test_reviews = test_reviews.apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果在max_df中超过百分之50的一个词其中大部分都在positive，positive indicator？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Naive bayes model\n",
    " https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method that take in the training dataset, then return the positive and negative words-log probability dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method that take in the training dataset, then return the positive and negative words log probability.\n",
    "Input: train_reviews: reviews (sentences) for training\n",
    "       train_sentiments: sentiments (label) for training\n",
    "       tfidf: boolean variable indicating whether using bow or tfidf\n",
    "       alpha: laplance smoothing variable, default to be 1.0\n",
    "       ngram_range: the scale of ngram model will be used, default = (1,1) unigram\n",
    "return: negative_word_log_prob_dict: dictionary that contains the word:log probability pair for negative class\n",
    "        positive_word_log_prob_dict: dictionary that contains the word:log probability pair for positive class\n",
    "        mnb: the trained multinomial naive bayes model, later can be used for testing\n",
    "        transformed_test_reviews: transformed test reviews that later can be used for testing\n",
    "        vec: either the tfidfVectorize build from tfidf model or the CountVectorizer build from Bag of word model.\n",
    "'''\n",
    "\n",
    "def generate_log_prob(train_reviews, train_sentiments, tfidf=False, alpha=1.0, ngram_range = (1,1)):\n",
    "\n",
    "    if (tfidf):\n",
    "        #Tfidf vectorizer\n",
    "        vec=TfidfVectorizer(use_idf=tfidf, ngram_range=ngram_range)\n",
    "        #transformed train reviews\n",
    "        transformed_train_reviews=vec.fit_transform(train_reviews)\n",
    "        #transformed test reviews\n",
    "        transformed_test_reviews=vec.transform(test_reviews)\n",
    "    else:\n",
    "        vec=CountVectorizer(ngram_range=(1,1))\n",
    "        transformed_train_reviews=vec.fit_transform(train_reviews)\n",
    "        transformed_test_reviews=vec.transform(test_reviews)\n",
    "\n",
    "    #training the model\n",
    "    mnb = MultinomialNB(alpha=alpha)\n",
    "\n",
    "    #fitting the naive bayes for bag of words\n",
    "    mnb = mnb.fit(transformed_train_reviews, train_sentiments)\n",
    "    negative_log_prob = mnb.feature_log_prob_[0]\n",
    "    positive_log_prob = mnb.feature_log_prob_[1]\n",
    "\n",
    "    # Generate two dict: word:log_prob\n",
    "    negative_word_log_prob_dict = {}\n",
    "    positive_word_log_prob_dict = {}\n",
    "    for word, index in vec.vocabulary_.items():\n",
    "        negative_word_log_prob_dict[word] = negative_log_prob[index]\n",
    "        positive_word_log_prob_dict[word] = positive_log_prob[index]\n",
    "    \n",
    "    return negative_word_log_prob_dict, positive_word_log_prob_dict, mnb, transformed_test_reviews, vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method that given input word, manually change the weight of the word in naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This method will take in a word:scale dict, then take in the negative and positive word:log_probability dict, manually change the weight of the words in the model and the dict\n",
    "Input: word_change_scale: this is the word-scale dictionary, how much the weight of the word should be changed, For example if the value is 0.5, we will say \n",
    "                          the probability of the word in negative class should multiply 0.5, in original probability, we take power to the scale\n",
    "       model: the trained naive bayes model, which the feature_log_prob_ attribute will be manually changed based on previous two params\n",
    "       negative_word_log_prob_dict: dictionary that contains the word:log probability pair for negative class, which some values will be changed\n",
    "       positive_word_log_prob_dict: dictionary that contains the word:log probability pair for positive class, which some values will be changed\n",
    "       vec: either the tfidfVectorize build from tfidf model or the CountVectorizer build from Bag of word model.\n",
    "return: negative_word_change_scale: The modified negative dict\n",
    "        positive_word_change_scale: The modified positive dict\n",
    "        model: The modified naive bayes model\n",
    "'''\n",
    "\n",
    "def change_weight(word_change_scale, model, negative_word_log_prob_dict, positive_word_log_prob_dict, vec):\n",
    "    for word, scale in word_change_scale.items():\n",
    "        # change the weight of words in negative and positive word:log_prob dict\n",
    "        negative_word_log_prob_dict[word] *= scale\n",
    "        positive_word_log_prob_dict[word] *= scale\n",
    "\n",
    "        # change the weight of words in the model\n",
    "        index_in_model = vec.vocabulary_[word]\n",
    "        model.feature_log_prob_[0][index_in_model] *= scale\n",
    "        model.feature_log_prob_[1][index_in_model] *= scale\n",
    "\n",
    "    return negative_word_log_prob_dict, positive_word_log_prob_dict, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the result using bag of word methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos, mnb, transformed_test_reviews, vec= generate_log_prob(train_reviews, train_sentiments, alpha = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block represent the length of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vec.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following two blocks shows the word:log_prob key value pair for each phrase in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following block use the trained model to do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_bow_predict = mnb.predict(transformed_test_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the corresponding test reviews log probability and original probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTICE: this probability was being normalized across all classes, so the log_probability was not the summation of log class probability and log probability of all phrases in the sentence. You can see that the summation of predict_proba() method is 1. The predict_log_proba() method calculate the log probability based on the normalized one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.predict_proba(transformed_test_reviews[0]), mnb.predict_log_proba(transformed_test_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This predict() method will return the predict value of given transformed review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.predict(transformed_test_reviews[0]), mnb.predict(transformed_test_reviews[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class_log_prior_ attribute of the MultinomialNB() model gives the prior class probability. In the order of the classes_ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.classes_, mnb.class_log_prior_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put the entire training set into the trained Naive Bayes model, and ge the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_bow_predict = mnb.predict(transformed_test_reviews)\n",
    "mnb_bow_score = accuracy_score(test_sentiments, mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_bow_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_labeled_NB_BOW = \"\"\n",
    "\n",
    "for i in range(mnb_bow_predict.size):\n",
    "    start_index = test_sentiments.index.start\n",
    "    if(mnb_bow_predict[i] != test_sentiments[start_index + i]):\n",
    "        wrong_labeled_NB_BOW += str(start_index + i) + \" \" + str(imdb_data['sentiment'][start_index + i]) + \" | \" + str(imdb_data['review'][start_index + i]) +  \"\\n\"\n",
    "\n",
    "f = open(\"Naive Bayes - Bag of Words Wrongly labeled sentences.txt\", \"w\")\n",
    "f.write(wrong_labeled_NB_BOW)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the model using TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos, mnb, transformed_test_reviews, vec = generate_log_prob(train_reviews, train_sentiments, tfidf = True, alpha = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tfidf_predict = mnb.predict(transformed_test_reviews)\n",
    "mnb_tfidf_score = accuracy_score(test_sentiments, mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report for TF-IDF\n",
    "mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_labeled_NB_TFIDF = \"\"\n",
    "for i in range(mnb_tfidf_predict.size):\n",
    "    start_index = test_sentiments.index.start\n",
    "    if(mnb_tfidf_predict[i] != test_sentiments[start_index + i]):\n",
    "        wrong_labeled_NB_TFIDF += str(start_index + i) + \" \" + str(imdb_data['sentiment'][start_index + i]) + \" | \" + str(imdb_data['review'][start_index + i]) +  \"\\n\"\n",
    "\n",
    "f = open(\"Naive Bayes - TF-IDF Wrongly labeled sentences.txt\", \"w\")\n",
    "f.write(wrong_labeled_NB_TFIDF)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "test_reviews.index.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #preprocess the data so it can feed into fasttext\n",
    "# def transform_instance(doc, label):\n",
    "#     processed_text = []\n",
    "#     #Prefix the index-ed label with __label__\n",
    "#     for i in range(doc.index.start, doc.index.stop):\n",
    "#         cur_row = \"__label__\" + label[i] + \" \" + doc[i]\n",
    "#         processed_text.append(cur_row)\n",
    "#     return processed_text\n",
    "\n",
    "# training_text = transform_instance(train_reviews, train_sentiments)\n",
    "# test_text = transform_instance(test_reviews, test_sentiments)\n",
    "\n",
    "# # Put the training and test dataset into file, so that the fasttext model can read them.\n",
    "# f = open(\"fasttext.train\", \"w\")\n",
    "# output = \"\"\n",
    "# for text in training_text:\n",
    "#     output += text\n",
    "#     output += \"\\n\"\n",
    "# f.write(output)\n",
    "# f.close()\n",
    "\n",
    "# f = open(\"fasttext.test\", \"w\")\n",
    "# output = \"\"\n",
    "# for text in test_text:\n",
    "#     output += text\n",
    "#     output += \"\\n\"\n",
    "# f.write(output)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input = \"fasttext.train\", lr=0.1, epoch=45, wordNgrams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model_movie.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(\"fasttext.test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the misclassified examples by fasttext algorithm\n",
    "wrong_labeled_fasttext = \"\"\n",
    "for i in range(test_reviews.size):\n",
    "    start_index = test_sentiments.index.start\n",
    "    if(model.predict(test_reviews[start_index + i])[0][0] != str('__label__' + test_sentiments[start_index + i])):\n",
    "        wrong_labeled_fasttext += str(start_index + i) + \" \" + str(imdb_data['sentiment'][start_index + i]) + \" | \" + str(imdb_data['review'][start_index + i]) +  \"\\n\"\n",
    "\n",
    "f = open(\"fastText Wrongly labeled sentences.txt\", \"w\")\n",
    "f.write(wrong_labeled_fasttext)\n",
    "f.close\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext manually modify the sentence embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def output_train_reviews_to_doc(train_reviews):\n",
    "#     processed_doc = \"\"\n",
    "#     for review in train_reviews:\n",
    "#         processed_doc += review + \"\\n\"\n",
    "#     return processed_doc\n",
    "\n",
    "# f = open(\"wordRepresentation.train\", \"w\")\n",
    "# f.write(output_train_reviews_to_doc(train_reviews))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised(\"wordRepresentation.train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_word_vector(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each train and test review, use all the embeddings of words, calculate an average to represent the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence_embedding(data, word_list):\n",
    "    representations = []\n",
    "    for sentence in data:\n",
    "        nltk_tokens = nltk.word_tokenize(sentence)\n",
    "        sum = 0\n",
    "        count = 0\n",
    "        for token in nltk_tokens:\n",
    "            if token not in word_list:\n",
    "                sum += model.get_word_vector(token)\n",
    "                count += 1\n",
    "        representations.append(sum / count)\n",
    "    return representations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label(data):\n",
    "    representations = []\n",
    "    for label in data:\n",
    "        if(label == 'negative'):\n",
    "            representations.append(0)\n",
    "        else:\n",
    "            representations.append(1)\n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "fasttext_train_reviews = torch.Tensor(np.array(transform_sentence_embedding(train_reviews, word_list)))\n",
    "fasttext_test_reviews = torch.Tensor(np.array(transform_sentence_embedding(test_reviews, word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train_sentiments = nn.functional.one_hot(torch.Tensor(np.array(transform_label(train_sentiments))).to(torch.int64)).to(torch.float)\n",
    "fasttext_test_sentiments = nn.functional.one_hot(torch.Tensor(np.array(transform_label(test_sentiments))).to(torch.int64)).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train_reviews.shape, fasttext_train_sentiments.shape, fasttext_test_reviews.shape, fasttext_test_sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "\n",
    "        # Non-linearity\n",
    "        self.sigmoid_1 = nn.Sigmoid()\n",
    "\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "        # Non-linearity\n",
    "        self.sigmoid_2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function  # LINEAR\n",
    "        out = self.fc1(x)\n",
    "\n",
    "        # Non-linearity  # NON-LINEAR\n",
    "        out = self.sigmoid_1(out)\n",
    "\n",
    "        # Linear function (readout)  # LINEAR\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        out = self.sigmoid_2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100\n",
    "hidden_dim = 100\n",
    "output_dim = 2\n",
    "\n",
    "learning_rate = 0.1\n",
    "n_epochs = 40\n",
    "batch_size = 32\n",
    "\n",
    "net = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "# Placeholder for loss\n",
    "track_loss = []\n",
    "\n",
    "print('Epoch', '\\t', 'Loss train', '\\t', 'Loss test')\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    # shuffle the permutation, so that the batches each time are different\n",
    "    shuffle_idx = np.random.permutation(len(fasttext_train_reviews))\n",
    "    #split the tensor into smaller batches\n",
    "    review_batches = torch.split(fasttext_train_reviews[shuffle_idx], batch_size)\n",
    "    sentiment_batches = torch.split(fasttext_train_sentiments[shuffle_idx], batch_size)\n",
    "    # Mini batches\n",
    "    for j in range(len(review_batches)):\n",
    "        review_batch = review_batches[j]\n",
    "\n",
    "        sentiment_batch = sentiment_batches[j]\n",
    "        \n",
    "        #where we train and build the network\n",
    "        output_train = net(review_batch)\n",
    "\n",
    "\n",
    "        loss = criterion(output_train, sentiment_batch)\n",
    "    \n",
    "        # compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Keep track of loss at each epoch\n",
    "    track_loss += [float(loss)]\n",
    "\n",
    "    loss_epoch = f'{i + 1} / {n_epochs}'\n",
    "    with torch.no_grad():\n",
    "        output_train = net(fasttext_train_reviews)\n",
    "        loss_train = criterion(output_train, fasttext_train_sentiments)\n",
    "        loss_epoch += f'\\t {loss_train:.4f}'\n",
    "\n",
    "        output_test = net(fasttext_test_reviews)\n",
    "        loss_test = criterion(output_test, fasttext_test_sentiments)\n",
    "        loss_epoch += f'\\t\\t {loss_test:.4f}'\n",
    "\n",
    "    print(loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_val = net(fasttext_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = []\n",
    "for (neg_prob, pos_prob) in predict_val:\n",
    "    if (neg_prob > pos_prob):\n",
    "        predict = \"negative\"\n",
    "    else:\n",
    "        predict = \"positive\"\n",
    "    predict_result.append(predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn_score = accuracy_score(test_sentiments, predict_result)\n",
    "ffnn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60d1cc28a4b92593bd5dc5bc27ccc3190a35517554a34efe05fccaa5f4e07df9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
