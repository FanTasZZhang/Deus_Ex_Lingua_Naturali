{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/williamchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/williamchen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import string \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data = pd.read_csv('./IMDB_Dataset.csv')\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews=imdb_data.review[:30000]\n",
    "train_sentiments=imdb_data.sentiment[:30000]\n",
    "\n",
    "val_reviews=imdb_data.review[30000:40000]\n",
    "val_sentiments=imdb_data.sentiment[30000:40000]\n",
    "\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(reviews):\n",
    "    container = []\n",
    "    for review in reviews:\n",
    "        review = review.replace(\"<br />\", \"\")\n",
    "        for ele in string.punctuation:\n",
    "                if ele in review:\n",
    "                        review = review.replace(ele, \"\")\n",
    "        container.append(review)\n",
    "    return container\n",
    "\n",
    "def preprocess_sentiment(sentiments):\n",
    "    container = []\n",
    "    for sentiment in sentiments:\n",
    "        container.append(sentiment)\n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = preprocess(train_reviews)\n",
    "val_reviews = preprocess(val_reviews)\n",
    "test_reviews = preprocess(test_reviews)\n",
    "train_sentiments = preprocess_sentiment(train_sentiments)\n",
    "val_sentiments = preprocess_sentiment(val_sentiments)\n",
    "test_sentiments = preprocess_sentiment(test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Method that take in the training dataset, then return the positive and negative words log probability.\n",
    "Input: train_reviews: reviews (sentences) for training\n",
    "       train_sentiments: sentiments (label) for training\n",
    "       val_reviews: reviews (sentences) for validation\n",
    "       test_reviews: reviews (sentences) for testing\n",
    "       tfidf: boolean variable indicating whether using bow or tfidf\n",
    "       alpha: laplance smoothing variable, default to be 1.0\n",
    "       ngram_range: the scale of ngram model will be used, default = (1,1) unigram\n",
    "return: negative_word_log_prob_dict: dictionary that contains the word:log probability pair for negative class\n",
    "        positive_word_log_prob_dict: dictionary that contains the word:log probability pair for positive class\n",
    "        mnb: the trained multinomial naive bayes model, later can be used for testing\n",
    "        transformed_val_reviews: transformed val reviews that later can be used for validation\n",
    "        transformed_test_reviews: transformed test reviews that later can be used for testing\n",
    "        vec: either the tfidfVectorize build from tfidf model or the CountVectorizer build from Bag of word model.\n",
    "'''\n",
    "\n",
    "def generate_log_prob(train_reviews, train_sentiments,  test_reviews, val_reviews=None, tfidf=False, alpha=1.0, ngram_range = (1,1)):\n",
    "\n",
    "    if (tfidf):\n",
    "        #Tfidf vectorizer\n",
    "        vec=TfidfVectorizer(use_idf=tfidf, ngram_range=ngram_range)\n",
    "        transformed_train_reviews=vec.fit_transform(train_reviews)\n",
    "        if val_reviews is not None:\n",
    "            transformed_val_reviews = vec.transform(val_reviews)\n",
    "        transformed_test_reviews=vec.transform(test_reviews)\n",
    "    else:\n",
    "        vec=CountVectorizer(ngram_range=(1,1))\n",
    "        transformed_train_reviews=vec.fit_transform(train_reviews)\n",
    "        if val_reviews is not None:\n",
    "            transformed_val_reviews = vec.transform(val_reviews)\n",
    "        transformed_test_reviews=vec.transform(test_reviews)\n",
    "\n",
    "    #training the model\n",
    "    mnb = MultinomialNB(alpha=alpha)\n",
    "\n",
    "    #fitting the naive bayes for bag of words\n",
    "    mnb = mnb.fit(transformed_train_reviews, train_sentiments)\n",
    "    \n",
    "    negative_log_prob = mnb.feature_log_prob_[0]\n",
    "    positive_log_prob = mnb.feature_log_prob_[1]\n",
    "\n",
    "    # Generate two dict: word:log_prob\n",
    "    negative_word_log_prob_dict = {}\n",
    "    positive_word_log_prob_dict = {}\n",
    "    for word, index in vec.vocabulary_.items():\n",
    "        negative_word_log_prob_dict[word] = negative_log_prob[index]\n",
    "        positive_word_log_prob_dict[word] = positive_log_prob[index]\n",
    "    if val_reviews is None:\n",
    "        return negative_word_log_prob_dict, positive_word_log_prob_dict, mnb, transformed_test_reviews, vec\n",
    "    else:\n",
    "        return negative_word_log_prob_dict, positive_word_log_prob_dict, mnb, transformed_val_reviews, transformed_test_reviews, vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reweight_dict(pos, neg, dataset, threshold=0.1):\n",
    "    wrongly_classified_dict = {} # counts how many times a token has negative impact on wrongly classified sentence\n",
    "    token_dict = {} # counts how many times a token has appeared in total\n",
    "    reweight_dict = {}\n",
    "    for label, sentence in dataset:\n",
    "        real_label = label\n",
    "        tokens = sentence\n",
    "        for token in tokens:\n",
    "            if token not in token_dict:\n",
    "                token_dict[token] = 1\n",
    "            else:\n",
    "                token_dict[token] += 1\n",
    "        if real_label == \"positive\": # marked as negative\n",
    "            for token in tokens:\n",
    "                if token not in pos: continue\n",
    "                elif pos[token] < neg[token]:\n",
    "                    if token not in wrongly_classified_dict:\n",
    "                        wrongly_classified_dict[token] = 1\n",
    "                    else:\n",
    "                        wrongly_classified_dict[token] += 1\n",
    "        elif real_label == \"negative\": # marked as positive\n",
    "            for token in tokens:\n",
    "                if token not in pos: continue\n",
    "                elif neg[token] < pos[token]:\n",
    "                    if token not in wrongly_classified_dict:\n",
    "                        wrongly_classified_dict[token] = 1\n",
    "                    else:\n",
    "                        wrongly_classified_dict[token] += 1\n",
    "    # print(wrongly_classified_token_dict)\n",
    "    for word, prob in wrongly_classified_dict.items():\n",
    "        # print(word, prob)\n",
    "        weight = 0\n",
    "        if pos[word] < neg[word]:\n",
    "            weight = pos[word] - neg[word]\n",
    "        elif neg[word] < pos[word]:\n",
    "            weight = neg[word] - pos[word]\n",
    "        multiplier = 1 - wrongly_classified_dict[word] / token_dict[word]\n",
    "        if multiplier == 0:\n",
    "            reweight_dict[word] = 0\n",
    "        else:\n",
    "            reweight_dict[word] = np.exp(weight) * multiplier\n",
    "            # print(\"{}:{} {}\".format(word, weight, multiplier))\n",
    "            # print(\"{}:{} -> {}\".format(word, pos[word] - neg[word], reweight_dict[word]*pos[word] - reweight_dict[word]*neg[word]))\n",
    "    return reweight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This method will take in a word:scale dict, then take in the negative and positive word:log_probability dict, manually change the weight of the words in the model and the dict\n",
    "Input: word_change_scale: this is the word-scale dictionary, how much the weight of the word should be changed, For example if the value is 0.5, we will say \n",
    "                          the probability of the word in negative class should multiply 0.5, in original probability, we take power to the scale\n",
    "       model: the trained naive bayes model, which the feature_log_prob_ attribute will be manually changed based on previous two params\n",
    "       negative_word_log_prob_dict: dictionary that contains the word:log probability pair for negative class, which some values will be changed\n",
    "       positive_word_log_prob_dict: dictionary that contains the word:log probability pair for positive class, which some values will be changed\n",
    "       vec: either the tfidfVectorize build from tfidf model or the CountVectorizer build from Bag of word model.\n",
    "return: negative_word_change_scale: The modified negative dict\n",
    "        positive_word_change_scale: The modified positive dict\n",
    "        model: The modified naive bayes model\n",
    "'''\n",
    "\n",
    "def change_weight(word_change_scale, model, negative_word_log_prob_dict, positive_word_log_prob_dict, vec):\n",
    "    for word, scale in word_change_scale.items():\n",
    "        # change the weight of words in negative and positive word:log_prob dict\n",
    "        negative_word_log_prob_dict[word] *= scale\n",
    "        positive_word_log_prob_dict[word] *= scale\n",
    "\n",
    "        # change the weight of words in the model\n",
    "        index_in_model = vec.vocabulary_[word]\n",
    "        model.feature_log_prob_[0][index_in_model] *= scale\n",
    "        model.feature_log_prob_[1][index_in_model] *= scale\n",
    "\n",
    "    return negative_word_log_prob_dict, positive_word_log_prob_dict, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Word - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos, mnb, transformed_val_reviews, transformed_test_reviews, vec= generate_log_prob(train_reviews, train_sentiments, test_reviews, val_reviews=val_reviews, alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.032443811652895 -15.049387153559886\n"
     ]
    }
   ],
   "source": [
    "unseen_neg = mnb.feature_log_prob_[0].min()\n",
    "unseen_pos = mnb.feature_log_prob_[1].min()\n",
    "print(unseen_neg, unseen_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweight_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8441\n",
      "0.8472\n",
      "val accuracy is 0.8441, test accuracy is 0.8472\n"
     ]
    }
   ],
   "source": [
    "val_pred, val_acc = naive_inference(reweight_dict, pos, neg, unseen_neg, unseen_pos, val_reviews, val_sentiments)\n",
    "test_pred, test_acc = naive_inference(reweight_dict, pos, neg, unseen_neg, unseen_pos, test_reviews, test_sentiments)\n",
    "print(f\"val accuracy is {val_acc}, test accuracy is {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_labeled = []\n",
    "pattern = r'[^A-Za-z0-9]+'\n",
    "\n",
    "for i in range(len(val_reviews)):\n",
    "    start_index = 30000\n",
    "    if(val_pred[i] != val_sentiments[i]):\n",
    "        sentiment = val_sentiments[i]\n",
    "        review = str(imdb_data['review'][start_index + i])\n",
    "        review = re.sub(pattern, \" \", review.lower()).split()\n",
    "        # review = test_reviews[i].strip().lower().split()\n",
    "        wrong_labeled.append((sentiment, review))\n",
    "\n",
    "reweight_dict = calculate_reweight_dict(pos, neg, wrong_labeled)\n",
    "# neg, pos, mnb = change_weight(reweight_dict, mnb, neg, pos, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9031\n",
      "0.841\n",
      "val accuracy is 0.9031, test accuracy is 0.841\n"
     ]
    }
   ],
   "source": [
    "val_pred_2, val_acc_2 = naive_inference(reweight_dict, pos, neg, unseen_neg, unseen_pos, val_reviews, val_sentiments)\n",
    "test_pred_2, test_acc_2 = naive_inference(reweight_dict, pos, neg, unseen_neg, unseen_pos, test_reviews, test_sentiments)\n",
    "print(f\"val accuracy is {val_acc_2}, test accuracy is {test_acc_2}\")\n",
    "\n",
    "# for i in range(len(val_pred)):\n",
    "#     if val_pred[i] != val_pred_2[i]:\n",
    "#         tokens = str(imdb_data['review'][start_index + i])\n",
    "#         tokens = re.sub(pattern, \" \", review.lower()).split()\n",
    "#         for token in tokens:\n",
    "#             weight = 1\n",
    "#             pprob = unseen_pos\n",
    "#             nprob = unseen_neg\n",
    "#             if word in reweight:\n",
    "#                 weight = reweight[word]\n",
    "#             if word in pos:\n",
    "#                 pprob = pos[word]\n",
    "#             if word in neg:\n",
    "#                 nprob = neg[word]\n",
    "#             final_result += weight*pprob - weight*nprob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_inference(reweight, pos, neg, unseen_neg, unseen_pos, test_reviews, test_sentiments):\n",
    "    correct = 0\n",
    "    prediction = []\n",
    "    for i in range(len(test_reviews)):\n",
    "        word_list = test_reviews[i]\n",
    "        word_list = re.sub(pattern, \" \", word_list.lower()).split()\n",
    "        # word_list = test_reviews[i].strip().lower().split()\n",
    "        final_result = 0\n",
    "        \n",
    "        for word in word_list:\n",
    "            weight = 1\n",
    "            pprob = unseen_pos\n",
    "            nprob = unseen_neg\n",
    "            if word in reweight:\n",
    "                weight = reweight[word]\n",
    "            if word in pos:\n",
    "                pprob = pos[word]\n",
    "            if word in neg:\n",
    "                nprob = neg[word]\n",
    "            final_result += weight*pprob - weight*nprob\n",
    "        if final_result > 0:\n",
    "            prediction.append(\"positive\")\n",
    "        elif final_result < 0:\n",
    "            prediction.append(\"negative\")\n",
    "        if (final_result > 0 and test_sentiments[i] == \"positive\") or (final_result < 0 and test_sentiments[i] == \"negative\"):\n",
    "            correct += 1\n",
    "    print(correct/len(test_reviews))\n",
    "    return prediction, correct/len(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    start_index = 40000\n",
    "    if(test_pred[i] != test_sentiments[i]):\n",
    "        sentiment = str(imdb_data['sentiment'][start_index + i])\n",
    "        review = str(imdb_data['review'][start_index + i])\n",
    "        review = re.sub(pattern, \" \", review.lower())\n",
    "        wrong_labeled.append((sentiment, review))\n",
    "\n",
    "reweight_dict = calculate_reweight_dict(pos, neg, wrong_labeled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweight the model using perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_update(positive_prob_dict, negative_prob_dict, unseen_pos, unseen_neg, val_reviews, val_sentiments, test_reviews, test_sentiments, max_iter=10, learning_rate=0.02):\n",
    "    \"\"\"\n",
    "    need to handle unseen word here\n",
    "    has to be a method\n",
    "    \"\"\"\n",
    "    initial_weight = {}\n",
    "    for i in range(max_iter):\n",
    "        random_array = np.arange(len(val_sentiments))\n",
    "        np.random.shuffle(random_array)\n",
    "        correct = 0\n",
    "        for index in random_array:\n",
    "            word_list = val_reviews[index]\n",
    "            word_list = re.sub(pattern, \" \", word_list.lower()).split()\n",
    "            # word_list = val_reviews[index].strip().lower().split()\n",
    "            final_result = 0\n",
    "            for word in word_list:\n",
    "                weight = 1\n",
    "                pprob = unseen_pos\n",
    "                nprob = unseen_neg\n",
    "                if word not in initial_weight:\n",
    "                    initial_weight[word] = 1\n",
    "                if word in initial_weight:\n",
    "                    weight = initial_weight[word]\n",
    "                if word in positive_prob_dict:\n",
    "                    pprob = positive_prob_dict[word]\n",
    "                if word in negative_prob_dict:\n",
    "                    nprob = negative_prob_dict[word]\n",
    "                final_result += weight*pprob - weight*nprob\n",
    "            if (final_result > 0 and val_sentiments[index] == \"positive\") or (final_result < 0 and val_sentiments[index] == \"negative\"):\n",
    "                correct += 1\n",
    "                for word in word_list:\n",
    "                    initial_weight[word] -= learning_rate/len(val_sentiments)*initial_weight[word]\n",
    "            else:\n",
    "                for word in word_list:\n",
    "                    pprob = unseen_pos\n",
    "                    nprob = unseen_neg\n",
    "                    if word in positive_prob_dict:\n",
    "                        pprob = positive_prob_dict[word]\n",
    "                    if word in negative_prob_dict:\n",
    "                        nprob = negative_prob_dict[word]\n",
    "                    if val_sentiments[index] == \"positive\":\n",
    "                        initial_weight[word] += learning_rate*((pprob-nprob) - 1/len(val_sentiments)*initial_weight[word])\n",
    "                    else:\n",
    "                        initial_weight[word] += learning_rate*((nprob-pprob) - 1/len(val_sentiments)*initial_weight[word])\n",
    "        test_pred, test_acc = naive_inference(initial_weight, positive_prob_dict, negative_prob_dict, unseen_neg, unseen_pos, test_reviews, test_sentiments)\n",
    "        print(f\"finish training epoch {i}, the val accuracy is {correct/len(val_sentiments)}, the test accuracy is {test_acc}\")\n",
    "        \n",
    "    return initial_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8734\n",
      "finish training epoch 0, the val accuracy is 0.8635, the test accuracy is 0.8734\n",
      "0.8817\n",
      "finish training epoch 1, the val accuracy is 0.881, the test accuracy is 0.8817\n",
      "0.8841\n",
      "finish training epoch 2, the val accuracy is 0.8939, the test accuracy is 0.8841\n",
      "0.8848\n",
      "finish training epoch 3, the val accuracy is 0.9042, the test accuracy is 0.8848\n",
      "0.8855\n",
      "finish training epoch 4, the val accuracy is 0.9119, the test accuracy is 0.8855\n",
      "0.8858\n",
      "finish training epoch 5, the val accuracy is 0.9167, the test accuracy is 0.8858\n",
      "0.8846\n",
      "finish training epoch 6, the val accuracy is 0.9243, the test accuracy is 0.8846\n",
      "0.8855\n",
      "finish training epoch 7, the val accuracy is 0.93, the test accuracy is 0.8855\n",
      "0.8861\n",
      "finish training epoch 8, the val accuracy is 0.9354, the test accuracy is 0.8861\n",
      "0.8843\n",
      "finish training epoch 9, the val accuracy is 0.9434, the test accuracy is 0.8843\n"
     ]
    }
   ],
   "source": [
    "reweight = perceptron_update(pos, neg, unseen_pos, unseen_neg, val_reviews, val_sentiments, test_reviews, test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_printed_version = dict(sorted(reweight.items(), key=lambda item: item[1]))\n",
    "with open(\"weight_dict.txt\", \"w\") as f:\n",
    "    for k, v in weight_printed_version.items():\n",
    "        f.write(str(k))\n",
    "        f.write(\": \")\n",
    "        f.write(str(v))\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('weight.pickle', 'wb') as handle:\n",
    "    pickle.dump(reweight, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60d1cc28a4b92593bd5dc5bc27ccc3190a35517554a34efe05fccaa5f4e07df9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
